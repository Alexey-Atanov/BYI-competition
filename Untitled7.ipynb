{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4455c820-f71a-43bd-a5b3-64fe6d92af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.8.0.dev20250409+cu128\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: D:\\Anaconda3\\envs\\my_env\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.3\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.25.2\n",
      "scipy version: 1.15.2\n",
      "Pillow version: 11.0.0\n",
      "Tensorboard version: 2.19.0\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.22.0.dev20250410+cu128\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: 1.6.2\n",
      "psutil version: 7.0.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.22.0\n",
      "pynrrd version: 1.1.3\n",
      "clearml version: 2.0.0rc0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n",
      "Using device: cuda\n",
      "Simulated dataset appears to exist at Downloads\\byu-locating-bacterial-flagellar-motors-2025\\train with matching GT Downloads\\byu-locating-bacterial-flagellar-motors-2025\\train_labels.csv. Skipping creation.\n",
      "Train Tomo IDs: ['tomo_000', 'tomo_001', 'tomo_002']\n",
      "Val Tomo IDs: ['tomo_003']\n",
      "Test Tomo IDs: ['tomo_004']\n",
      "Attempting to create train_ds_base with 3 tomograms: ['tomo_000', 'tomo_001', 'tomo_002']\n",
      "Train dataset size: 3\n",
      "Attempting to get first item from train_loader_base...\n",
      "Shape of first training batch 'image': torch.Size([4, 1, 96, 96, 96])\n",
      "Shape of first training batch 'label': torch.Size([4, 1, 96, 96, 96])\n",
      "Attempting to create val_ds_base with 1 tomograms: ['tomo_003']\n",
      "Validation dataset size: 1\n",
      "Attempting to get first item from val_loader_base...\n",
      "Shape of first validation item 'image': torch.Size([1, 1, 134, 116, 116])\n",
      "Shape of first validation item 'label': torch.Size([1, 1, 134, 116, 116])\n",
      "WORKFLOW: Starting Hyperparameter Optimization (Optuna)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 21:33:52,239] Using an existing study with name 'CryoEM_MotorMetaNet_study' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Trial 4: Starting with params {'base_model_lr': 4.865150593642991e-05, 'gnn_node_candidate_threshold': 0.6454186578345281, 'gnn_edge_max_distance': 35, 'gnn_gnn_lr': 0.0011339926912395115, 'gnn_hidden_channels': 32, 'gnn_num_layers': 2, 'gnn_gat_heads': 4}\n",
      "--- Stage 1: Training Base Model ---\n",
      "Starting base model training for 1 epochs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78211386bbe74db1b0e5161ccbea5346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 Training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss: 0.2642\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c2bfc76d584a0f91628740927d4ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation Dice: 0.0011\n",
      "Saved new best model to output_cryoEM_MotorMetaNet\\DynUNet_best_epoch1_dice0.0011.pth\n",
      "Finished base model training. Best validation metric: 0.0011 at epoch 1\n",
      "Loaded best model from output_cryoEM_MotorMetaNet\\DynUNet_best_epoch1_dice0.0011.pth for GNN data generation.\n",
      "--- Stage 2: Generating GNN Training Data ---\n",
      "Generating GNN training data for 1 tomograms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11214ac0dd3849fc8066292c2b86e2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Tomos for GNN Data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 graphs for GNN training.\n",
      "Not enough GNN graphs for training split. GNN training skipped.\n",
      "Trial 4: Model training failed to produce models. Reporting low score.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 21:33:55,848] Trial 4 finished with value: -1.0 and parameters: {'base_model_lr': 4.865150593642991e-05, 'gnn_node_candidate_threshold': 0.6454186578345281, 'gnn_edge_max_distance': 35, 'gnn_gnn_lr': 0.0011339926912395115, 'gnn_hidden_channels': 32, 'gnn_num_layers': 2, 'gnn_gat_heads': 4}. Best is trial 0 with value: -1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Trial 5: Starting with params {'base_model_lr': 2.9824524172137357e-05, 'gnn_node_candidate_threshold': 0.4623229900911201, 'gnn_edge_max_distance': 11, 'gnn_gnn_lr': 0.0013344400450179684, 'gnn_hidden_channels': 64, 'gnn_num_layers': 2, 'gnn_gat_heads': 2}\n",
      "--- Stage 1: Training Base Model ---\n",
      "Starting base model training for 1 epochs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71efc958233249f585ea2a04ba1f8b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 Training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss: 0.2650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35bbb68961a4b74b21fcc3ea1223091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation Dice: 0.0011\n",
      "Saved new best model to output_cryoEM_MotorMetaNet\\DynUNet_best_epoch1_dice0.0011.pth\n",
      "Finished base model training. Best validation metric: 0.0011 at epoch 1\n",
      "Loaded best model from output_cryoEM_MotorMetaNet\\DynUNet_best_epoch1_dice0.0011.pth for GNN data generation.\n",
      "--- Stage 2: Generating GNN Training Data ---\n",
      "Generating GNN training data for 1 tomograms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed7d51dea56482d983c5f3998237d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Tomos for GNN Data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 graphs for GNN training.\n",
      "Not enough GNN graphs for training split. GNN training skipped.\n",
      "Trial 5: Model training failed to produce models. Reporting low score.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 21:33:59,362] Trial 5 finished with value: -1.0 and parameters: {'base_model_lr': 2.9824524172137357e-05, 'gnn_node_candidate_threshold': 0.4623229900911201, 'gnn_edge_max_distance': 11, 'gnn_gnn_lr': 0.0013344400450179684, 'gnn_hidden_channels': 64, 'gnn_num_layers': 2, 'gnn_gat_heads': 2}. Best is trial 0 with value: -1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Study Summary:\n",
      "  Number of finished trials: 6\n",
      "  Best trial number: 0\n",
      "  Best F2-score: -1.0000\n",
      "  Best parameters:\n",
      "    base_model_lr: 1.0740586090703404e-05\n",
      "    gnn_node_candidate_threshold: 0.5136299758870757\n",
      "    gnn_edge_max_distance: 12\n",
      "    gnn_gnn_lr: 0.003515395317060771\n",
      "    gnn_hidden_channels: 32\n",
      "    gnn_num_layers: 2\n",
      "    gnn_gat_heads: 2\n",
      "Best parameters saved to output_cryoEM_MotorMetaNet\\best_optuna_params.json\n",
      "\n",
      "Optuna finished. Best parameters found:\n",
      "{\n",
      "  \"base_model_lr\": 1.0740586090703404e-05,\n",
      "  \"gnn_node_candidate_threshold\": 0.5136299758870757,\n",
      "  \"gnn_edge_max_distance\": 12,\n",
      "  \"gnn_gnn_lr\": 0.003515395317060771,\n",
      "  \"gnn_hidden_channels\": 32,\n",
      "  \"gnn_num_layers\": 2,\n",
      "  \"gnn_gat_heads\": 2\n",
      "}\n",
      "Best F2-score from HPO: -1.0\n",
      "Global CONFIG updated with best HPO parameters.\n",
      "\n",
      "WORKFLOW: Starting Full Training Pipeline with current CONFIG...\n",
      "--- Stage 1: Training Base Model ---\n",
      "Starting base model training for 2 epochs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c913df973f264b61ae057c26b1cde0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2 Training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss: 0.2659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934a9548fede4314a1956034123e44fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation Dice: 0.0011\n",
      "Saved new best model to output_cryoEM_MotorMetaNet\\DynUNet_best_epoch1_dice0.0011.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025ea0bdf32f436d873f061f79da5770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2 Training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average training loss: 0.2645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af814347a324d8f865feb91bbea44d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation Dice: 0.0011\n",
      "Saved new best model to output_cryoEM_MotorMetaNet\\DynUNet_best_epoch2_dice0.0011.pth\n",
      "Finished base model training. Best validation metric: 0.0011 at epoch 2\n",
      "Loaded best model from output_cryoEM_MotorMetaNet\\DynUNet_best_epoch2_dice0.0011.pth for GNN data generation.\n",
      "--- Stage 2: Generating GNN Training Data ---\n",
      "Generating GNN training data for 1 tomograms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2aff6cf89c4c82985a4ac8aaaf7c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Tomos for GNN Data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 graphs for GNN training.\n",
      "Not enough GNN graphs for training split. GNN training skipped.\n",
      "Full training pipeline complete (or partially complete if GNN failed).\n",
      "Saved final base model to output_cryoEM_MotorMetaNet\\final_trained_base_model.pth\n",
      "GNN model specified and PyG available, but GNN model not trained/returned.\n",
      "\n",
      "WORKFLOW: Starting Inference...\n",
      "\n",
      "Running 3D inference on tomogram: tomo_004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 181.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing inference using only base model (GNN not available or specified).\n",
      "Found 500 detections in 3D for tomo_004.\n",
      "Evaluation for 3D detections on tomo_004: {'f2_score': 0.00992063491079302, 'precision': 0.0019999999995999997, 'recall': 0.9999999000000099, 'tp': 1, 'fp': 499, 'fn': 0}\n",
      "\n",
      "Running 2D (adapted) inference on slice: slice_0060 from tomo_004\n",
      "Performing 2D adapted inference using only base model for slice slice_0060.\n",
      "Found 200 detections in 2D for slice slice_0060.\n",
      "\n",
      "CryoEM-MotorMetaNet Workflow Demo Finished.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # CryoEM-MotorMetaNet: A GNN-Stacked Ensemble for 3D Bacterial Motor Detection in Cryo-ET\n",
    "#\n",
    "# ## Overall Goal\n",
    "# Develop a state-of-the-art 3D deep learning pipeline, delivered as a single, comprehensive Jupyter Notebook (.ipynb file), to automatically detect and localize bacterial motors within 3D cryo-electron tomography (cryo-ET) reconstructions. The training data consists of folders of 2D image slices (JPG format). The core detection will be performed by one or more base 3D CNN models, and their outputs will be intelligently combined and refined using a Graph Neural Network (GNN) acting as a stacking meta-model. The system must address challenges of extremely small motor size, low SNR, and the need for precise 3D localization. Optimization will be driven by the F-beta score (beta=2.0) using Optuna, with MONAI for data augmentation and other pipeline components. A critical contingency is handling potentially 2D slice-based test data by adapting the entire 3D-trained pipeline.\n",
    "#\n",
    "# ## 1. Context and Motivation\n",
    "# Bacterial flagellar motors are crucial nanomachines. Cryo-ET offers 3D visualization, but manual analysis is a bottleneck. This project aims to create an automated system leveraging 3D CNNs for robust feature extraction and a GNN meta-model for advanced ensembling of base model predictions. This sophisticated stacking approach is expected to enhance detection accuracy for these challenging, small targets. The entire solution is encapsulated in this notebook and designed to be adaptable for different test data formats.\n",
    "#\n",
    "# ## 2. Input Data Detailed Description\n",
    "# *   **Training/Validation Data:**\n",
    "#     *   Format: Each 3D tomogram is a sequence of 2D JPG slices.\n",
    "#     *   Structure: Slices for one tomogram are in a folder (e.g., `tomo_003acc/slice_0000.jpg`, `.../slice_NNNN.jpg`).\n",
    "#     *   3D Volume: Slices are ordered along the Z-axis, typically 8-bit grayscale.\n",
    "# *   **Test Data Contingency:**\n",
    "#     *   While training uses 3D tomograms, the test set might be individual 2D slices. The solution includes adaptation for this.\n",
    "# *   **Data Characteristics:** Low Signal-to-Noise Ratio (SNR), low contrast, extremely small target size, variable appearance, potential anisotropy.\n",
    "# *   **Dataset Size (Illustrative - User to define):**\n",
    "#     *   Number of tomograms (train/val/test): e.g., 50 train, 10 val, 10 test.\n",
    "#     *   Slices/tomogram: e.g., 100-500.\n",
    "#     *   X-Y dimensions: e.g., 512x512 or 1024x1024.\n",
    "#     *   Class imbalance: Motors are sparse.\n",
    "# *   **Ground Truth (for 3D Tomograms - Training/Validation):**\n",
    "#     *   Format: CSV file.\n",
    "#     *   Assumed CSV Structure: `tomo_id,center_x,center_y,center_z` (plus potentially a `row_id`). Coordinates are voxel indices.\n",
    "#     *   Target Generation: Since motors are small, ground truth for base CNNs will be small 3D spherical/cubical masks centered at these (X,Y,Z) coordinates. Bounding boxes are not assumed to be directly provided in the GT CSV but will be generated for output.\n",
    "#\n",
    "# ## 3. Desired Output Format\n",
    "# *   **For 3D Tomogram Input:**\n",
    "#     *   Output: List of 3D detections per tomogram.\n",
    "#     *   Each detection: 3D Coordinates (center: X,Y,Z), 3D Bounding Box (X_min,Y_min,Z_min,X_max,Y_max,Z_max), Confidence Score.\n",
    "# *   **For 2D Slice Input (Adapted):**\n",
    "#     *   Output: List of 2D detections per slice.\n",
    "#     *   Each detection: 2D Coordinates (center on slice: X,Y), 2D Bounding Box (on slice: X_min,Y_min,X_max,Y_max), Confidence Score.\n",
    "#\n",
    "# ## 4. Proposed Methodology: Stacked Ensemble with GNN Meta-Model\n",
    "#\n",
    "# ### 4.1. Base Model(s): 3D Convolutional Neural Networks (3D CNNs)\n",
    "# *   **Purpose:** Extract features and generate initial candidate detections or probability maps from 3D tomogram patches.\n",
    "# *   **Number:** N=1 base model (MONAI DynUNet) will be implemented for simplicity in this notebook, but the framework can be extended.\n",
    "# *   **Training:** Trained on 3D tomogram patches to produce 3D probability maps for motor presence.\n",
    "# *   **Output:** A 3D probability map indicating motor likelihood at each voxel.\n",
    "#\n",
    "# ### 4.2. GNN as a Stacking Meta-Model\n",
    "# *   **Purpose:** Combine predictions/features from base model(s) to make a refined prediction, leveraging relational information between candidate detections.\n",
    "# *   **Graph Construction:**\n",
    "#     1.  **Node Definition:** Candidate motor regions identified from base model output (e.g., peaks in probability maps after Non-Maximum Suppression - NMS).\n",
    "#     2.  **Node Features:** Derived from base model(s) output (e.g., probability scores, local intensity statistics).\n",
    "#     3.  **Edge Definition:** Connect nodes based on spatial proximity (e.g., Euclidean distance).\n",
    "# *   **GNN Architecture:** Graph Attention Network (GAT) is suitable due to its ability to weigh neighbor importance.\n",
    "# *   **GNN Task:** Node Classification (motor vs. non-motor).\n",
    "# *   **Training the GNN:** Two-stage: Train base model(s) first, then use their outputs on a validation set to create a dataset for training the GNN.\n",
    "#\n",
    "# ### 4.3. Overall Pipeline Flow\n",
    "# 1.  Input 3D tomogram (or patch).\n",
    "# 2.  Base 3D CNN Model processes input, generating a probability map.\n",
    "# 3.  Construct a graph: nodes are candidate detections from the base model, features from base model output.\n",
    "# 4.  GNN Meta-Model processes the graph, outputting refined detections.\n",
    "#\n",
    "# ## Notebook Structure:\n",
    "# *   Part 0: Setup and Imports\n",
    "# *   Part 1: Configuration and Global Parameters\n",
    "# *   Part 2: Data Loading and Preprocessing (MONAI)\n",
    "# *   Part 3: Base 3D CNN Model (MONAI DynUNet)\n",
    "# *   Part 4: GNN Meta-Model (PyTorch Geometric - GAT)\n",
    "# *   Part 5: Training Orchestration (Base Model + GNN)\n",
    "# *   Part 6: Evaluation Metrics and Utilities\n",
    "# *   Part 7: Inference Pipeline (3D Tomograms and 2D Slices)\n",
    "# *   Part 8: Hyperparameter Optimization with Optuna\n",
    "# *   Part 9: Usage Instructions\n",
    "# *   Part 10: Discussion, Challenges, and Future Work\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 0: Setup and Imports\n",
    "# Ensure you have a Python environment with CUDA-enabled PyTorch.\n",
    "\n",
    "# %%\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Or your CUDA version\n",
    "# !pip install monai[all]==1.3.0 # Using a specific version for stability\n",
    "# !pip install optuna==3.5.0\n",
    "# !pip install torch_geometric pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)')+cu118.html # Adjust cuXXX\n",
    "# !pip install pandas scikit-image scikit-learn matplotlib opencv-python joblib tqdm\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 # OpenCV for image loading if needed, though MONAI's LoadImage is preferred\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.data import (\n",
    "    Dataset as MonaiDataset,\n",
    "    DataLoader as MonaiDataLoader,\n",
    "    ImageReader,\n",
    "    PersistentDataset,\n",
    "    CacheDataset,\n",
    "    list_data_collate,\n",
    "    pad_list_data_collate\n",
    ")\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, ScaleIntensityRanged, NormalizeIntensityd,\n",
    "    Spacingd, RandSpatialCropSamplesd, RandFlipd, RandRotate90d, RandAffined,\n",
    "    Rand3DElasticd, RandGaussianNoised, RandAdjustContrastd, ToTensord,\n",
    "    Activationsd, AsDiscreted, KeepLargestConnectedComponentd, LabelToContourD,\n",
    "    CropForegroundd\n",
    ")\n",
    "from monai.networks.nets import DynUNet, UNet\n",
    "from monai.losses import DiceCELoss, DiceLoss, FocalLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference, SimpleInferer\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "# PyTorch Geometric\n",
    "try:\n",
    "    import torch_geometric\n",
    "    import torch_geometric.nn as pyg_nn\n",
    "    from torch_geometric.data import Data as PyGData\n",
    "    from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "    PYG_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyTorch Geometric not found. GNN functionality will be disabled.\")\n",
    "    PYG_AVAILABLE = False\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import label, regionprops\n",
    "from scipy.ndimage import center_of_mass, binary_dilation, binary_erosion\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Print MONAI config\n",
    "print_config()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 1: Configuration and Global Parameters\n",
    "\n",
    "# %%\n",
    "# %% [markdown]\n",
    "# ## Part 1: Configuration and Global Parameters\n",
    "\n",
    "# %%\n",
    "CONFIG = {\n",
    "    # General Paths and Data Settings\n",
    "    \"project_name\": \"CryoEM_MotorMetaNet\",\n",
    "    \"dataset_root\": \"./Downloads/byu-locating-bacterial-flagellar-motors-2025/train\", # Path to root of dataset\n",
    "    \"gt_csv_path\": \"./Downloads/byu-locating-bacterial-flagellar-motors-2025/train_labels.csv\", # Path to ground truth CSV\n",
    "    \"output_dir\": \"./output_cryoEM_MotorMetaNet\", # Where to save models, logs, HPO results\n",
    "\n",
    "    # Data characteristics (user may need to adjust)\n",
    "    \"num_slices_per_tomo_avg\": 200, # Approximate, for patch size considerations\n",
    "    \"xy_dimensions_avg\": (512, 512), # Approximate\n",
    "\n",
    "    # Training Parameters - Base Model\n",
    "    \"base_model_name\": \"DynUNet\",\n",
    "    \"patch_size_base\": (96, 96, 96), # XYZ patch size for training base model\n",
    "    \"base_model_target_radius\": 3, # Radius of sphere for GT mask generation (voxels)\n",
    "    \"base_model_train_epochs\": 2, # Low for demo; increase for real training (e.g., 100-300)\n",
    "    \"base_model_val_interval\": 1, # Low for demo\n",
    "    \"base_model_lr\": 1e-4,\n",
    "    \"base_model_batch_size\": 1, # Limited by GPU VRAM with large patches\n",
    "    \"num_samples_per_volume\": 4, # For RandSpatialCropSamplesd\n",
    "    \"num_base_models\": 1, # For this demo, N=1. Can be extended.\n",
    "\n",
    "    # Training Parameters - GNN Meta-Model\n",
    "    \"gnn_model_name\": \"GAT\",\n",
    "    \"gnn_node_candidate_threshold\": 0.3, # Probability threshold to consider a peak a candidate node\n",
    "    \"gnn_node_NMS_footprint\": (5,5,5), # Footprint for NMS for candidate generation\n",
    "    \"gnn_edge_max_distance\": 20, # Max distance (voxels) to connect nodes in graph\n",
    "    \"gnn_train_epochs\": 2, # Low for demo; increase (e.g., 50-100)\n",
    "    \"gnn_lr\": 1e-3,\n",
    "    \"gnn_batch_size\": 1, # Number of graphs per batch\n",
    "    \"gnn_hidden_channels\": 64,\n",
    "    \"gnn_num_layers\": 3,\n",
    "    \"gnn_gat_heads\": 4,\n",
    "\n",
    "    # Inference Parameters\n",
    "    \"inference_roi_size_base\": (96, 96, 96), # Sliding window ROI for base model\n",
    "    \"inference_sw_batch_size_base\": 2, # Sliding window batch size\n",
    "    \"inference_overlap_base\": 0.5,\n",
    "    \"output_bbox_size_3d\": (11, 11, 11), # Odd numbers for centered box\n",
    "    \"output_bbox_size_2d\": (11, 11),\n",
    "\n",
    "    # HPO Parameters\n",
    "    \"optuna_n_trials\": 3, # Low for demo; increase significantly (e.g., 50-100+)\n",
    "    \"optuna_objective_metric\": \"f2_score\", # F-beta with beta=2\n",
    "\n",
    "    # System\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    # MODIFICATION: Set num_workers to 0 for debugging DataLoader issues\n",
    "    \"num_workers\": 0, # Was 2, Changed to 0 for debugging\n",
    "    \"random_seed\": 42,\n",
    "\n",
    "    # For 2D slice inference adaptation\n",
    "    \"pseudo_3d_depth_for_2d_slice\": 32,\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"dataset_root\"]).mkdir(parents=True, exist_ok=True) # For simulated data\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_determinism(CONFIG[\"random_seed\"])\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "if not PYG_AVAILABLE and CONFIG[\"gnn_model_name\"]:\n",
    "    print(\"WARNING: PyTorch Geometric not available, GNN part will not function correctly.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Utility: Simulate Dataset Creation\n",
    "# This function will create dummy JPG slice folders and a ground truth CSV file.\n",
    "# **Replace this with your actual data loading logic if you have the dataset.**\n",
    "\n",
    "# %%\n",
    "def simulate_dataset(root_dir, gt_csv_path, num_tomos, slices_per_tomo_range, dims, num_motors_range):\n",
    "    # Ensure root_dir and gt_csv_path are Path objects for consistency\n",
    "    root_dir = Path(root_dir)\n",
    "    gt_csv_path = Path(gt_csv_path)\n",
    "\n",
    "    # Check if the specific GT file exists and the number of tomo folders matches\n",
    "    # This check should be robust to whether root_dir was passed with a trailing slash or not\n",
    "    tomo_folders_exist = list(root_dir.glob(\"tomo_*\"))\n",
    "\n",
    "    if gt_csv_path.exists() and len(tomo_folders_exist) == num_tomos :\n",
    "        # Further check: do these tomo_folders actually contain JPGs? (Simplified check here)\n",
    "        # This check is basic; a more robust one would verify content or a marker file.\n",
    "        first_tomo_path_check = root_dir / tomo_folders_exist[0].name if tomo_folders_exist else None\n",
    "        if first_tomo_path_check and list(first_tomo_path_check.glob(\"*.jpg\")):\n",
    "\n",
    "            print(f\"Simulated dataset appears to exist at {root_dir} with matching GT {gt_csv_path}. Skipping creation.\")\n",
    "            try:\n",
    "                gt_df = pd.read_csv(gt_csv_path)\n",
    "                tomo_ids = sorted(list(gt_df['tomo_id'].unique()))\n",
    "                if len(tomo_ids) == num_tomos: # Ensure CSV also reflects the correct number of tomos\n",
    "                    return tomo_ids\n",
    "                else:\n",
    "                    print(f\"Warning: GT CSV tomo_ids count ({len(tomo_ids)}) doesn't match expected num_tomos ({num_tomos}). Recreating.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing GT CSV or validating: {e}. Will recreate.\")\n",
    "        else:\n",
    "            print(f\"Tomogram folders found, but content check failed or first tomogram is empty. Recreating dataset.\")\n",
    "\n",
    "\n",
    "    print(f\"Simulating dataset at {root_dir}...\")\n",
    "    # If we are recreating, clean up existing simulated data in that specific location first\n",
    "    if root_dir.exists():\n",
    "        # Be careful with recursive deletion. Only delete if it's the intended simulation dir.\n",
    "        # For safety, only delete tomo_* folders and the gt_csv file.\n",
    "        for item in root_dir.glob(\"tomo_*\"):\n",
    "            if item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "            elif item.is_file(): # Should not happen for tomo_*\n",
    "                item.unlink()\n",
    "        if gt_csv_path.exists():\n",
    "            gt_csv_path.unlink()\n",
    "        print(f\"Cleaned up existing simulated data in {root_dir} for recreation.\")\n",
    "\n",
    "\n",
    "    root_dir.mkdir(parents=True, exist_ok=True) # Ensure root exists\n",
    "    all_gt_data = []\n",
    "    tomo_ids = []\n",
    "\n",
    "    for i in range(num_tomos):\n",
    "        tomo_id = f\"tomo_{i:03d}\"\n",
    "        tomo_ids.append(tomo_id)\n",
    "        tomo_path = root_dir / tomo_id\n",
    "        tomo_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        num_slices = random.randint(slices_per_tomo_range[0], slices_per_tomo_range[1])\n",
    "        height, width = dims\n",
    "\n",
    "        for s_idx in range(num_slices):\n",
    "            slice_img = np.random.randint(0, 256, size=(height, width), dtype=np.uint8)\n",
    "            if s_idx > num_slices // 3 and s_idx < 2 * num_slices // 3 :\n",
    "                 slice_img[height//4:3*height//4, width//4:3*width//4] = np.clip(slice_img[height//4:3*height//4, width//4:3*width//4] + random.randint(-30,30),0,255)\n",
    "            cv2.imwrite(str(tomo_path / f\"slice_{s_idx:04d}.jpg\"), slice_img)\n",
    "\n",
    "        num_motors = random.randint(num_motors_range[0], num_motors_range[1])\n",
    "        for m_idx in range(num_motors):\n",
    "            center_z = random.randint(CONFIG[\"base_model_target_radius\"] + 1, num_slices - CONFIG[\"base_model_target_radius\"] -1 )\n",
    "            center_y = random.randint(CONFIG[\"base_model_target_radius\"] + 1, height - CONFIG[\"base_model_target_radius\"]-1)\n",
    "            center_x = random.randint(CONFIG[\"base_model_target_radius\"] + 1, width - CONFIG[\"base_model_target_radius\"]-1)\n",
    "            all_gt_data.append({\n",
    "                \"row_id\": f\"{tomo_id}_motor_{m_idx:03d}\",\n",
    "                \"tomo_id\": tomo_id,\n",
    "                \"center_x\": center_x,\n",
    "                \"center_y\": center_y,\n",
    "                \"center_z\": center_z\n",
    "            })\n",
    "\n",
    "    gt_df = pd.DataFrame(all_gt_data)\n",
    "    gt_df.to_csv(gt_csv_path, index=False)\n",
    "    print(f\"Simulated dataset created. GT at {gt_csv_path}\")\n",
    "    return sorted(tomo_ids)\n",
    "\n",
    "# MODIFICATION: Use user's provided paths from the traceback for simulation\n",
    "# This ensures the simulation matches where the error likely occurred.\n",
    "# The traceback indicates paths like:\n",
    "# GT: .\\Downloads\\byu-locating-bacterial-flagellar-motors-2025\\train_labels.csv\n",
    "# Dataset root (implied by GT path): .\\Downloads\\byu-locating-bacterial-flagellar-motors-2025\n",
    "# These are relative paths; their absolute location depends on the CWD.\n",
    "# For consistency, I will use these relative paths as strings.\n",
    "USER_DATASET_ROOT = r\".\\Downloads\\byu-locating-bacterial-flagellar-motors-2025\\train\"\n",
    "USER_GT_CSV_PATH = r\".\\Downloads\\byu-locating-bacterial-flagellar-motors-2025\\train_labels.csv\"\n",
    "\n",
    "# Update CONFIG to use these paths for simulation and dataset loading\n",
    "CONFIG[\"dataset_root\"] = USER_DATASET_ROOT\n",
    "CONFIG[\"gt_csv_path\"] = USER_GT_CSV_PATH\n",
    "\n",
    "SIM_NUM_TOMOS = 5\n",
    "SIM_SLICES_PER_TOMO = (CONFIG[\"patch_size_base\"][2] + 20, CONFIG[\"patch_size_base\"][2] + 50)\n",
    "SIM_DIMS = (CONFIG[\"patch_size_base\"][0] + 20, CONFIG[\"patch_size_base\"][1] + 20)\n",
    "SIM_NUM_MOTORS = (1, 5)\n",
    "\n",
    "ALL_TOMO_IDS = simulate_dataset(\n",
    "    CONFIG[\"dataset_root\"],\n",
    "    CONFIG[\"gt_csv_path\"],\n",
    "    SIM_NUM_TOMOS,\n",
    "    SIM_SLICES_PER_TOMO,\n",
    "    SIM_DIMS,\n",
    "    SIM_NUM_MOTORS\n",
    ")\n",
    "\n",
    "if len(ALL_TOMO_IDS) >= 5:\n",
    "    TRAIN_TOMO_IDS = ALL_TOMO_IDS[:3]\n",
    "    VAL_TOMO_IDS = ALL_TOMO_IDS[3:4]\n",
    "    TEST_TOMO_IDS = ALL_TOMO_IDS[4:5]\n",
    "elif len(ALL_TOMO_IDS) >=3:\n",
    "    TRAIN_TOMO_IDS = ALL_TOMO_IDS[:1]\n",
    "    VAL_TOMO_IDS = ALL_TOMO_IDS[1:2]\n",
    "    TEST_TOMO_IDS = ALL_TOMO_IDS[2:3]\n",
    "else:\n",
    "    TRAIN_TOMO_IDS = ALL_TOMO_IDS\n",
    "    VAL_TOMO_IDS = ALL_TOMO_IDS\n",
    "    TEST_TOMO_IDS = ALL_TOMO_IDS\n",
    "\n",
    "print(f\"Train Tomo IDs: {TRAIN_TOMO_IDS}\")\n",
    "print(f\"Val Tomo IDs: {VAL_TOMO_IDS}\")\n",
    "print(f\"Test Tomo IDs: {TEST_TOMO_IDS}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 2: Data Loading and Preprocessing (MONAI)\n",
    "#\n",
    "# ### 2.1. Custom ImageReader for JPG Sequences\n",
    "\n",
    "# %%\n",
    "# MODIFICATION: Add traceback import\n",
    "import traceback\n",
    "\n",
    "import traceback\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "import glob\n",
    "import os\n",
    "import cv2 # Ensure cv2 is imported\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "# Required for type hints in verify_suffix, ensure these are available\n",
    "from typing import Union, Sequence \n",
    "# from os import PathLike # More specific: Union[str, Path] is often sufficient\n",
    "\n",
    "class JPGSequenceReader(ImageReader):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def read(self, data: Union[str, Path, Sequence[Union[str, Path]]], **kwargs): # Adjusted type hint for data\n",
    "        # print(f\"[JPGSequenceReader] Reading from: {data}\") # Debug print\n",
    "        if isinstance(data, (list, tuple)):\n",
    "             img_files = sorted([str(f) for f in data])\n",
    "        elif Path(data).is_dir():\n",
    "            img_files = sorted(glob.glob(os.path.join(str(data), \"*.jpg\"))) # Ensure data is str for os.path.join\n",
    "        else:\n",
    "            # Check if 'data' is a single file path string that was expected to be a directory\n",
    "            if isinstance(data, (str, Path)) and not Path(data).exists():\n",
    "                 raise FileNotFoundError(f\"Input path {data} does not exist.\")\n",
    "            raise ValueError(f\"JPGSequenceReader expects a directory path or list of JPG files, got {type(data)}: {data}\")\n",
    "\n",
    "        if not img_files:\n",
    "            # print(f\"[JPGSequenceReader] No JPG files found in {data}\") # Debug print\n",
    "            raise FileNotFoundError(f\"No JPG files found in {data}\")\n",
    "        # print(f\"[JPGSequenceReader] Found {len(img_files)} files in {data}\") # Debug print\n",
    "\n",
    "        slices = []\n",
    "        expected_shape = None\n",
    "        for idx, img_file in enumerate(img_files):\n",
    "            # print(f\"[JPGSequenceReader] Reading slice: {img_file}\") # Debug print\n",
    "            slice_img = cv2.imread(img_file, cv2.IMREAD_GRAYSCALE)\n",
    "            if slice_img is None:\n",
    "                # print(f\"[JPGSequenceReader] ERROR: Could not read image {img_file}\") # Debug print\n",
    "                raise IOError(f\"Could not read image {img_file}\")\n",
    "            \n",
    "            if expected_shape is None:\n",
    "                expected_shape = slice_img.shape\n",
    "            elif slice_img.shape != expected_shape:\n",
    "                # print(f\"[JPGSequenceReader] ERROR: Slice {img_file} shape {slice_img.shape} differs from expected {expected_shape}\") # Debug print\n",
    "                raise ValueError(f\"Inconsistent slice dimensions in tomogram {data}. Expected {expected_shape}, got {slice_img.shape} for {img_file}\")\n",
    "            slices.append(slice_img)\n",
    "\n",
    "        if not slices: # Should be caught by 'if not img_files' earlier, but as a safeguard\n",
    "            raise RuntimeError(f\"No slices were loaded for {data}, though image files might have been listed.\")\n",
    "\n",
    "        volume = np.stack(slices, axis=-1) # Stack along Z: HxWxD\n",
    "        # print(f\"[JPGSequenceReader] Stacked volume shape: {volume.shape}\") # Debug print\n",
    "        \n",
    "        # Construct basic affine: Assuming voxel size 1,1,1 and origin 0,0,0 for simplicity\n",
    "        # MONAI expects affine in RAS+ coordinate system (Right, Anterior, Superior)\n",
    "        # If slices are HxWxD (Height, Width, Depth along Z)\n",
    "        # Image array is typically (X, Y, Z, ...)\n",
    "        # Let's assume data is (width, height, depth) for affine purposes.\n",
    "        # Affine: diagonal contains pixel/voxel spacing, last column is origin.\n",
    "        # For JPG, spacing is usually not encoded, assume 1.0.\n",
    "        # Default affine (identity for voxel space)\n",
    "        affine = np.eye(4) \n",
    "        # If your slices represent a physical volume, and you know the pixel spacing and slice thickness:\n",
    "        # pixel_spacing_x, pixel_spacing_y, slice_thickness_z = 1.0, 1.0, 1.0 # Example values\n",
    "        # affine = np.array([\n",
    "        #     [pixel_spacing_x, 0, 0, 0],\n",
    "        #     [0, pixel_spacing_y, 0, 0],\n",
    "        #     [0, 0, slice_thickness_z, 0],\n",
    "        #     [0, 0, 0, 1]\n",
    "        # ])\n",
    "        return volume, {\"original_affine\": affine, \"spatial_shape\": np.array(volume.shape)}\n",
    "\n",
    "    def get_data(self, img_obj): # img_obj is what read() returned\n",
    "        # ImageReader.get_data expects to return (array, metadata_dict)\n",
    "        return img_obj \n",
    "\n",
    "    # <<< BEGIN MODIFICATION >>>\n",
    "    def verify_suffix(self, filename: Union[str, Path, Sequence[Union[str, Path]]]) -> bool:\n",
    "        \"\"\"\n",
    "        Verify that the filename is a directory (assumed to contain JPGs),\n",
    "        or a .jpg/.jpeg file (if single file passed, though `read` handles directories primarily),\n",
    "        or a sequence of .jpg/.jpeg files.\n",
    "        \"\"\"\n",
    "        if isinstance(filename, (list, tuple)):\n",
    "            if not filename:  # Empty sequence\n",
    "                return False\n",
    "            return all(Path(f).suffix.lower() in (\".jpg\", \".jpeg\") for f in filename)\n",
    "        \n",
    "        # Handle single PathLike object (str or Path)\n",
    "        p_fn = Path(filename)\n",
    "        if p_fn.is_dir():\n",
    "            # The reader is designed to read JPG sequences from a directory.\n",
    "            return True \n",
    "        if p_fn.is_file():\n",
    "            return p_fn.suffix.lower() in (\".jpg\", \".jpeg\")\n",
    "        \n",
    "        # If filename is a string but not an existing file or directory path,\n",
    "        # it's ambiguous. For example, it could be a file pattern.\n",
    "        # Since `read` primarily expects an existing directory,\n",
    "        # we are stricter here. If it's not a recognized file/dir, return False.\n",
    "        return False\n",
    "    # <<< END MODIFICATION >>>\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.2. Ground Truth Processing and Dataset Definition\n",
    "\n",
    "# %%\n",
    "class CryoETDataset(MonaiDataset):\n",
    "    def __init__(self, data_root, tomo_ids, gt_csv_path, transforms, target_radius, is_train=True):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.tomo_ids = tomo_ids\n",
    "        # print(f\"[CryoETDataset] Initializing with gt_csv_path: {gt_csv_path}\") # Debug\n",
    "        try:\n",
    "            self.gt_df = pd.read_csv(gt_csv_path)\n",
    "            # print(f\"[CryoETDataset] Successfully loaded GT CSV. Columns: {self.gt_df.columns.tolist()}\") #Debug\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[CryoETDataset] ERROR: Ground truth CSV file not found at {gt_csv_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"[CryoETDataset] ERROR: Failed to load or parse GT CSV {gt_csv_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.target_radius = target_radius\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.data_files = []\n",
    "        for tomo_id in self.tomo_ids:\n",
    "            tomo_path = self.data_root / tomo_id\n",
    "            if tomo_path.is_dir() and list(tomo_path.glob(\"*.jpg\")): # Basic check\n",
    "                 self.data_files.append({\"image\": str(tomo_path), \"id\": tomo_id})\n",
    "            else:\n",
    "                print(f\"Warning: Tomogram directory {tomo_path} not found or empty for tomo_id '{tomo_id}'. Skipping.\")\n",
    "        \n",
    "        if not self.data_files and self.tomo_ids: # If tomo_ids were provided but no valid files found\n",
    "            raise ValueError(f\"No valid data files found for the provided tomo_ids. Check dataset_root ('{self.data_root}') and tomo_id subfolders.\")\n",
    "        # print(f\"[CryoETDataset] Initialized with {len(self.data_files)} data files.\") # Debug\n",
    "\n",
    "\n",
    "    def _create_target_mask(self, shape_3d, motor_coords):\n",
    "        # shape_3d is (D, H, W)\n",
    "        mask = np.zeros(shape_3d, dtype=np.float32)\n",
    "        # print(f\"[_create_target_mask] Mask shape: {shape_3d}, Num motor_coords: {len(motor_coords)}\") # Debug\n",
    "\n",
    "        for i, mc in enumerate(motor_coords):\n",
    "            # print(f\"[_create_target_mask] Processing motor coord {i}: {mc}\") # Debug\n",
    "            if len(mc) != 3:\n",
    "                print(f\"Warning: Motor coord {mc} is not of length 3. Skipping.\")\n",
    "                continue\n",
    "            try:\n",
    "                # Ensure they are integers for indexing\n",
    "                cz, cy, cx = int(round(mc[0])), int(round(mc[1])), int(round(mc[2])) # Z, Y, X\n",
    "            except (ValueError, TypeError) as e:\n",
    "                print(f\"Warning: Could not convert motor coord {mc} to int: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            if not (0 <= cz < shape_3d[0] and \\\n",
    "                    0 <= cy < shape_3d[1] and \\\n",
    "                    0 <= cx < shape_3d[2]):\n",
    "                print(f\"Warning: Motor coord ({cx},{cy},{cz}) out of bounds for shape {shape_3d[::-1]} (orig shape_3d D,H,W: {shape_3d}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            z_min, z_max = max(0, cz - self.target_radius), min(shape_3d[0], cz + self.target_radius + 1)\n",
    "            y_min, y_max = max(0, cy - self.target_radius), min(shape_3d[1], cy + self.target_radius + 1)\n",
    "            x_min, x_max = max(0, cx - self.target_radius), min(shape_3d[2], cx + self.target_radius + 1)\n",
    "            mask[z_min:z_max, y_min:y_max, x_min:x_max] = 1.0\n",
    "        return mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # MODIFICATION: Wrap major parts of __getitem__ in a try-except for better error reporting\n",
    "        data_item = {} # Initialize to ensure it exists in except block\n",
    "        try:\n",
    "            data_item = self.data_files[index].copy()\n",
    "            # print(f\"[__getitem__] Processing index {index}, tomo_id: {data_item.get('id', 'Unknown')}\") # Debug\n",
    "\n",
    "            img_arr, meta = JPGSequenceReader().read(data_item[\"image\"]) # HxWxD\n",
    "            img_arr = img_arr.transpose(2,0,1) # Transpose to DxHxW\n",
    "            data_item[\"image\"] = img_arr\n",
    "            data_item[\"image_meta_dict\"] = meta\n",
    "            # print(f\"[__getitem__] Loaded image shape (D,H,W): {img_arr.shape}\") # Debug\n",
    "\n",
    "            tomo_id_current = data_item[\"id\"]\n",
    "            tomo_gt = self.gt_df[self.gt_df[\"tomo_id\"] == tomo_id_current]\n",
    "            motor_coords_zyx = []\n",
    "            if not tomo_gt.empty:\n",
    "                # Ensure columns exist before trying to access\n",
    "                required_cols = [\"center_z\", \"center_y\", \"center_x\"]\n",
    "                if not all(col in tomo_gt.columns for col in required_cols):\n",
    "                    raise KeyError(f\"Ground truth CSV for tomo_id {tomo_id_current} is missing one of required columns: {required_cols}. Found: {tomo_gt.columns.tolist()}\")\n",
    "                \n",
    "                motor_coords_zyx = list(zip(tomo_gt[\"center_z\"].values,\n",
    "                                            tomo_gt[\"center_y\"].values,\n",
    "                                            tomo_gt[\"center_x\"].values))\n",
    "            # print(f\"[__getitem__] Found {len(motor_coords_zyx)} motors for {tomo_id_current}\") # Debug\n",
    "            \n",
    "            target_mask = self._create_target_mask(img_arr.shape, motor_coords_zyx)\n",
    "            data_item[\"label\"] = target_mask\n",
    "            # print(f\"[__getitem__] Created target_mask shape: {target_mask.shape}, sum: {np.sum(target_mask)}\") # Debug\n",
    "\n",
    "            # Apply transforms\n",
    "            # print(f\"[__getitem__] Applying transforms for {tomo_id_current}...\") # Debug\n",
    "            processed_data = self.transforms(data_item)\n",
    "            # if isinstance(processed_data, list): # RandSpatialCropSamplesd case\n",
    "            #     print(f\"[__getitem__] Transforms returned list of {len(processed_data)} samples.\") # Debug\n",
    "            #     for i, p_data in enumerate(processed_data):\n",
    "            #          print(f\"  Sample {i} image shape: {p_data['image'].shape}, label shape: {p_data['label'].shape}\") # Debug\n",
    "            # else: # Single item case\n",
    "            #     print(f\"[__getitem__] Transforms returned single item. Image shape: {processed_data['image'].shape}, label shape: {processed_data['label'].shape}\") # Debug\n",
    "\n",
    "            return processed_data\n",
    "\n",
    "        except Exception as e:\n",
    "            # This will now catch errors from JPG reading, GT processing, mask creation, or transforms\n",
    "            print(f\"ERROR in CryoETDataset.__getitem__ for index {index}, data_item id '{data_item.get('id', 'Unknown')}': {e}\")\n",
    "            traceback.print_exc() # Print full traceback\n",
    "            # To make DataLoader continue and not hang, can return None or a dummy item,\n",
    "            # but this often hides problems. For debugging, re-raising is better with num_workers=0.\n",
    "            raise e\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.3. MONAI Transforms\n",
    "\n",
    "# %%\n",
    "# For training base model (includes patch sampling and augmentation)\n",
    "train_transforms_base = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    RandSpatialCropSamplesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        roi_size=CONFIG[\"patch_size_base\"],\n",
    "        num_samples=CONFIG[\"num_samples_per_volume\"],\n",
    "        random_center=True,\n",
    "        random_size=False,\n",
    "    ),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, max_k=3, spatial_axes=(0,1)), # ZY plane (axis=2)\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, max_k=3, spatial_axes=(0,2)), # ZX plane (axis=1)\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, max_k=3, spatial_axes=(1,2)), # YX plane (axis=0) - MONAI uses (H,W) for spatial_axes=(0,1) on 3D\n",
    "\n",
    "    RandGaussianNoised(keys=[\"image\"], prob=0.1, mean=0.0, std=0.01),\n",
    "    RandAdjustContrastd(keys=[\"image\"], prob=0.1, gamma=(0.5, 1.5)),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# For validation base model (processes full volumes or large ROIs, no augmentation usually)\n",
    "val_transforms_base = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.4. DataLoaders\n",
    "\n",
    "# %%\n",
    "if TRAIN_TOMO_IDS:\n",
    "    print(f\"Attempting to create train_ds_base with {len(TRAIN_TOMO_IDS)} tomograms: {TRAIN_TOMO_IDS}\")\n",
    "    try:\n",
    "        train_ds_base = CryoETDataset(\n",
    "            data_root=CONFIG[\"dataset_root\"],\n",
    "            tomo_ids=TRAIN_TOMO_IDS,\n",
    "            gt_csv_path=CONFIG[\"gt_csv_path\"],\n",
    "            transforms=train_transforms_base,\n",
    "            target_radius=CONFIG[\"base_model_target_radius\"],\n",
    "            is_train=True\n",
    "        )\n",
    "        if len(train_ds_base) == 0: # Check if dataset ended up empty\n",
    "             print(\"Warning: train_ds_base is empty after initialization. No data will be loaded.\")\n",
    "             train_loader_base = None\n",
    "        else:\n",
    "            train_loader_base = MonaiDataLoader(\n",
    "                train_ds_base,\n",
    "                batch_size=CONFIG[\"base_model_batch_size\"],\n",
    "                shuffle=True,\n",
    "                num_workers=CONFIG[\"num_workers\"], # Will be 0 due to earlier change\n",
    "                collate_fn=list_data_collate,\n",
    "                pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "            print(f\"Train dataset size: {len(train_ds_base)}\")\n",
    "            if len(train_ds_base) > 0 and train_loader_base is not None: # Added check for loader not None\n",
    "                print(\"Attempting to get first item from train_loader_base...\")\n",
    "                # This is where the original error occurred\n",
    "                first_batch = first(train_loader_base)\n",
    "                if first_batch: # Check if first_batch is not None\n",
    "                    print(f\"Shape of first training batch 'image': {first_batch['image'].shape}\")\n",
    "                    print(f\"Shape of first training batch 'label': {first_batch['label'].shape}\")\n",
    "                else:\n",
    "                    print(\"first(train_loader_base) returned None. DataLoader might be empty or failed silently on first item.\")\n",
    "            elif train_loader_base is None:\n",
    "                 print(\"train_loader_base is None, skipping first item check.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to create train_ds_base or train_loader_base: {e}\")\n",
    "        traceback.print_exc()\n",
    "        train_loader_base = None # Ensure it's None on failure\n",
    "else:\n",
    "    train_loader_base = None\n",
    "    print(\"No training data specified (TRAIN_TOMO_IDS is empty).\")\n",
    "\n",
    "\n",
    "if VAL_TOMO_IDS:\n",
    "    print(f\"Attempting to create val_ds_base with {len(VAL_TOMO_IDS)} tomograms: {VAL_TOMO_IDS}\")\n",
    "    try:\n",
    "        val_ds_base = CryoETDataset(\n",
    "            data_root=CONFIG[\"dataset_root\"],\n",
    "            tomo_ids=VAL_TOMO_IDS,\n",
    "            gt_csv_path=CONFIG[\"gt_csv_path\"],\n",
    "            transforms=val_transforms_base,\n",
    "            target_radius=CONFIG[\"base_model_target_radius\"],\n",
    "            is_train=False\n",
    "        )\n",
    "        if len(val_ds_base) == 0:\n",
    "            print(\"Warning: val_ds_base is empty after initialization.\")\n",
    "            val_loader_base = None\n",
    "        else:\n",
    "            val_loader_base = MonaiDataLoader(\n",
    "                val_ds_base, \n",
    "                batch_size=1, # Typically 1 for validation of full volumes\n",
    "                shuffle=False, \n",
    "                num_workers=CONFIG[\"num_workers\"], # Will be 0\n",
    "                pin_memory=torch.cuda.is_available()\n",
    "                # No list_data_collate needed if not using RandSpatialCropSamplesd for val\n",
    "            )\n",
    "            print(f\"Validation dataset size: {len(val_ds_base)}\")\n",
    "            if len(val_ds_base) > 0 and val_loader_base is not None:\n",
    "                print(\"Attempting to get first item from val_loader_base...\")\n",
    "                first_val_item = first(val_loader_base)\n",
    "                if first_val_item:\n",
    "                    print(f\"Shape of first validation item 'image': {first_val_item['image'].shape}\")\n",
    "                    print(f\"Shape of first validation item 'label': {first_val_item['label'].shape}\")\n",
    "                else:\n",
    "                    print(\"first(val_loader_base) returned None.\")\n",
    "            elif val_loader_base is None:\n",
    "                 print(\"val_loader_base is None, skipping first item check.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to create val_ds_base or val_loader_base: {e}\")\n",
    "        traceback.print_exc()\n",
    "        val_loader_base = None\n",
    "else:\n",
    "    val_loader_base = None\n",
    "    print(\"No validation data specified (VAL_TOMO_IDS is empty).\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 3: Base 3D CNN Model (MONAI DynUNet)\n",
    "# Using DynUNet from MONAI, which is highly configurable.\n",
    "\n",
    "# %%\n",
    "def get_base_model(config):\n",
    "    # Пример: 5 уровней блоков (включая начальный и самый глубокий), 4 этапа downsampling\n",
    "    num_blocks = 5 # Это будет длина kernel_size, strides, filters\n",
    "\n",
    "    # Filters для каждого из 5 блоков\n",
    "    # Вы можете сделать это настраиваемым через config, если хотите\n",
    "    # Например: start_filters = config.get(\"dynunet_start_filters\", 32)\n",
    "    # num_blocks = config.get(\"dynunet_num_blocks\", 5)\n",
    "    # dynunet_filters = [start_filters * (2**i) for i in range(num_blocks)]\n",
    "    dynunet_filters = [32, 64, 128, 256, 512] \n",
    "    if len(dynunet_filters) != num_blocks:\n",
    "        # Если вы хотите гибко менять num_blocks, генерируйте фильтры соответственно\n",
    "        print(f\"Warning: Length of predefined dynunet_filters ({len(dynunet_filters)}) \"\n",
    "              f\"does not match num_blocks ({num_blocks}). Adjusting filters.\")\n",
    "        start_filters = dynunet_filters[0] if dynunet_filters else 32\n",
    "        dynunet_filters = [start_filters * (2**i) for i in range(num_blocks)]\n",
    "\n",
    "\n",
    "    # Strides: первый для начального блока, остальные для downsampling\n",
    "    # Первый stride обычно (1,1,1) или (1,2,2) если хотите уменьшить XY сразу.\n",
    "    # Остальные (2,2,2) для уменьшения вдвое на каждом этапе downsampling.\n",
    "    # Общая длина списка strides будет num_blocks.\n",
    "    initial_stride = config.get(\"dynunet_initial_stride\", (1,1,1)) # Позволяет настроить начальный stride\n",
    "    downsample_stride = config.get(\"dynunet_downsample_stride\", (2,2,2))\n",
    "    \n",
    "    actual_strides = [initial_stride] + [downsample_stride] * (num_blocks - 1)\n",
    "\n",
    "    # Kernel sizes: по одному для каждого блока. Длина также num_blocks.\n",
    "    # Обычно (3,3,3) для всех блоков.\n",
    "    kernel_dim = config.get(\"dynunet_kernel_dim\", 3)\n",
    "    unet_kernel_sizes = [(kernel_dim, kernel_dim, kernel_dim)] * num_blocks \n",
    "\n",
    "    # Upsample kernel size (на самом деле это upsample strides для TransposedConv)\n",
    "    # Их должно быть num_blocks - 1 (для количества операций upsampling)\n",
    "    # Значения должны соответствовать шагам downsampling (кроме первого initial_stride)\n",
    "    # т.е. если downsampling был (2,2,2), то и upsampling должен быть (2,2,2)\n",
    "    upsample_strides_values = [downsample_stride] * (num_blocks - 1)\n",
    "\n",
    "\n",
    "    # --- Проверки длин согласно требованиям DynUNet ---\n",
    "    if not (len(unet_kernel_sizes) == len(actual_strides) == len(dynunet_filters)):\n",
    "        # Эта ошибка не должна возникать, если логика выше верна\n",
    "        raise ValueError(\n",
    "            f\"CRITICAL INTERNAL ERROR: Lengths of kernel_size ({len(unet_kernel_sizes)}), \"\n",
    "            f\"strides ({len(actual_strides)}), and filters ({len(dynunet_filters)}) \"\n",
    "            f\"must be equal. Current num_blocks={num_blocks}.\"\n",
    "        )\n",
    "    \n",
    "    # Проверка длины upsample_kernel_size\n",
    "    # len(strides) - 1 == len(upsample_kernel_size)\n",
    "    # len(actual_strides) - 1 == len(upsample_strides_values)\n",
    "    expected_upsample_len = len(actual_strides) - 1 \n",
    "    if len(upsample_strides_values) != expected_upsample_len:\n",
    "         # Эта ошибка не должна возникать, если логика выше верна\n",
    "        raise ValueError(\n",
    "            f\"CRITICAL INTERNAL ERROR: Length of upsample_kernel_size ({len(upsample_strides_values)}) \"\n",
    "            f\"must be {expected_upsample_len} (which is len(strides) - 1).\"\n",
    "        )\n",
    "\n",
    "    # Условие \"no less than 3\" для kernel_size и strides (фактически для num_blocks)\n",
    "    if num_blocks < 3:\n",
    "        raise ValueError(f\"DynUNet requires at least 3 levels (num_blocks >= 3). Current num_blocks={num_blocks}.\")\n",
    "\n",
    "    model = DynUNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1, # Grayscale input\n",
    "        out_channels=1, # Output probability map (pre-sigmoid)\n",
    "        kernel_size=unet_kernel_sizes,\n",
    "        strides=actual_strides,\n",
    "        upsample_kernel_size=upsample_strides_values, # Передаем шаги для upsampling\n",
    "        filters=dynunet_filters,\n",
    "        norm_name=\"instance\",\n",
    "        act_name=(\"leakyrelu\", {\"negative_slope\": 0.01}),\n",
    "        deep_supervision=False, # Установите True, если хотите использовать глубокий надзор\n",
    "        # deep_supr_num= num_blocks - 2, # Обычно для deep_supervision (если num_blocks >=3)\n",
    "        res_block=config.get(\"dynunet_res_block\", True), # Сделать ResBlock настраиваемым\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Base Model Training Loop\n",
    "\n",
    "# %%\n",
    "def train_base_model(model, train_loader, val_loader, config, trial=None): # trial for Optuna\n",
    "    model.to(config[\"device\"])\n",
    "    \n",
    "    # Loss function (DiceCE is good for segmentation, Focal for sparse targets)\n",
    "    # loss_function = DiceCELoss(to_onehot_y=False, sigmoid=True, squared_pred=True, lambda_dice=0.5, lambda_ce=0.5)\n",
    "    loss_function = FocalLoss(to_onehot_y=False, gamma=2.0, reduction=\"mean\") # Sigmoid applied internally usually\n",
    "                                                                            # if not, apply sigmoid to model output.\n",
    "                                                                            # DynUNet output is raw logits.\n",
    "    # For FocalLoss, ensure model output is logits, then apply sigmoid before loss or use `include_background=False`\n",
    "    # and ensure labels are {0,1}. MONAI's FocalLoss expects input logits.\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"base_model_lr\"], weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    train_losses = []\n",
    "    val_metrics_list = [] # Store F-beta or Dice scores from validation\n",
    "\n",
    "    # Inferer for validation on full volumes\n",
    "    val_inferer = sliding_window_inference\n",
    "    roi_size_val = config[\"inference_roi_size_base\"] # Use same as training patch or larger if GPU allows\n",
    "    sw_batch_size_val = config[\"inference_sw_batch_size_base\"]\n",
    "\n",
    "    post_pred_val = Compose([Activationsd(keys=\"pred\", sigmoid=True), AsDiscreted(keys=\"pred\", threshold=0.5)])\n",
    "    post_label_val = Compose([AsDiscreted(keys=\"label\", threshold=0.5)]) # GT is already 0 or 1\n",
    "\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "\n",
    "    print(f\"Starting base model training for {config['base_model_train_epochs']} epochs.\")\n",
    "\n",
    "    for epoch in range(config[\"base_model_train_epochs\"]):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        # train_loader might be None if no train_ids\n",
    "        if not train_loader:\n",
    "            print(\"Skipping training epoch as train_loader is None.\")\n",
    "            break # Exit training loop if no data\n",
    "\n",
    "        for batch_data in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['base_model_train_epochs']} Training\"):\n",
    "            step += 1\n",
    "            inputs, labels = batch_data[\"image\"].to(config[\"device\"]), batch_data[\"label\"].to(config[\"device\"])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= step\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1} average training loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % config[\"base_model_val_interval\"] == 0:\n",
    "            model.eval()\n",
    "            if not val_loader:\n",
    "                print(\"Skipping validation as val_loader is None.\")\n",
    "                # If Optuna trial, report intermediate value based on training loss\n",
    "                if trial:\n",
    "                    trial.report(epoch_loss, epoch) # Report training loss if no validation\n",
    "                    if trial.should_prune():\n",
    "                        raise optuna.exceptions.TrialPruned()\n",
    "                continue # Continue to next epoch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_data in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "                    val_inputs = val_data[\"image\"].to(config[\"device\"])\n",
    "                    val_labels = val_data[\"label\"].to(config[\"device\"]) # Full label mask\n",
    "\n",
    "                    # Sliding window inference for full volume\n",
    "                    val_outputs = val_inferer(val_inputs, roi_size_val, sw_batch_size_val, model, overlap=CONFIG[\"inference_overlap_base\"], mode=\"gaussian\", progress=False)\n",
    "                    \n",
    "                    # Apply post-processing (sigmoid, threshold)\n",
    "                    processed_val_outputs = [post_pred_val({\"pred\": val_out})[\"pred\"] for val_out in val_outputs]\n",
    "                    processed_val_labels = [post_label_val({\"label\": val_lab})[\"label\"] for val_lab in val_labels]\n",
    "\n",
    "                    # Compute Dice metric (or F-beta later)\n",
    "                    dice_metric(y_pred=processed_val_outputs, y=processed_val_labels)\n",
    "\n",
    "                metric_val = dice_metric.aggregate().item()\n",
    "                dice_metric.reset()\n",
    "                val_metrics_list.append(metric_val) # Store Dice for now\n",
    "                scheduler.step(metric_val) # Or use train_loss for scheduler if val is too noisy / infrequent\n",
    "\n",
    "                print(f\"Epoch {epoch+1} validation Dice: {metric_val:.4f}\")\n",
    "\n",
    "                if metric_val > best_metric:\n",
    "                    best_metric = metric_val\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    # Save best model (can be more sophisticated, e.g. based on F-beta)\n",
    "                    model_save_path = Path(config[\"output_dir\"]) / f\"{config['base_model_name']}_best_epoch{best_metric_epoch}_dice{best_metric:.4f}.pth\"\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': epoch_loss,\n",
    "                        'metric': best_metric,\n",
    "                        'config': config # Save config used for this model\n",
    "                    }, model_save_path)\n",
    "                    print(f\"Saved new best model to {model_save_path}\")\n",
    "\n",
    "                # Optuna pruning\n",
    "                if trial:\n",
    "                    trial.report(metric_val, epoch) # Report validation metric to Optuna\n",
    "                    if trial.should_prune():\n",
    "                        torch.cuda.empty_cache()\n",
    "                        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    print(f\"Finished base model training. Best validation metric: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "    \n",
    "    # Load best model for subsequent steps if not already loaded\n",
    "    if best_metric_epoch != -1:\n",
    "        best_model_path = Path(config[\"output_dir\"]) / f\"{config['base_model_name']}_best_epoch{best_metric_epoch}_dice{best_metric:.4f}.pth\"\n",
    "        if best_model_path.exists():\n",
    "            checkpoint = torch.load(best_model_path, map_location=config[\"device\"])\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded best model from {best_model_path} for GNN data generation.\")\n",
    "        else:\n",
    "            print(f\"Warning: Best model path {best_model_path} not found. Using last epoch model.\")\n",
    "\n",
    "    return model, {\"train_losses\": train_losses, \"val_metrics\": val_metrics_list, \"best_val_metric\": best_metric, \"best_epoch\": best_metric_epoch}\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 4: GNN Meta-Model (PyTorch Geometric - GAT)\n",
    "#\n",
    "# ### 4.1. Graph Construction Utilities\n",
    "\n",
    "# %%\n",
    "def get_candidate_nodes_from_prob_map(prob_map_tensor, prob_threshold, nms_footprint_voxels, max_candidates=500):\n",
    "    \"\"\"\n",
    "    Extracts candidate motor locations (nodes) from a probability map.\n",
    "    Args:\n",
    "        prob_map_tensor (torch.Tensor): Single channel probability map (D, H, W) on CPU or GPU.\n",
    "        prob_threshold (float): Minimum probability to consider a voxel.\n",
    "        nms_footprint_voxels (tuple): Size of footprint for non-maximum suppression (Z,Y,X).\n",
    "        max_candidates (int): Max number of candidates to return.\n",
    "    Returns:\n",
    "        np.ndarray: Array of candidate coordinates (N, 3) in (z, y, x) order.\n",
    "        np.ndarray: Array of corresponding probabilities (N,).\n",
    "    \"\"\"\n",
    "    if prob_map_tensor.ndim == 4: # (C,D,H,W)\n",
    "        if prob_map_tensor.shape[0] != 1:\n",
    "            raise ValueError(\"Prob map tensor should be single channel for candidate extraction.\")\n",
    "        prob_map_tensor = prob_map_tensor.squeeze(0)\n",
    "    \n",
    "    prob_map_np = prob_map_tensor.cpu().numpy()\n",
    "\n",
    "    # Threshold\n",
    "    binary_map = prob_map_np > prob_threshold\n",
    "    if np.sum(binary_map) == 0:\n",
    "        return np.empty((0,3), dtype=int), np.empty((0,), dtype=float)\n",
    "\n",
    "    # Non-Maximum Suppression (simplified local max filter)\n",
    "    # A more robust NMS would use skimage.feature.peak_local_max or similar\n",
    "    # For simplicity, this is a basic NMS:\n",
    "    from scipy.ndimage import maximum_filter\n",
    "    local_max = maximum_filter(prob_map_np, footprint=np.ones(nms_footprint_voxels)) == prob_map_np\n",
    "    \n",
    "    # Combine binary map (threshold) and local maxima\n",
    "    candidate_mask = binary_map & local_max\n",
    "    \n",
    "    coords_zyx = np.argwhere(candidate_mask) # Get (z,y,x) coordinates of candidates\n",
    "    \n",
    "    if coords_zyx.shape[0] == 0:\n",
    "        return np.empty((0,3), dtype=int), np.empty((0,), dtype=float)\n",
    "        \n",
    "    candidate_probs = prob_map_np[coords_zyx[:,0], coords_zyx[:,1], coords_zyx[:,2]]\n",
    "\n",
    "    # Sort by probability and take top N\n",
    "    if len(candidate_probs) > max_candidates:\n",
    "        sorted_indices = np.argsort(candidate_probs)[::-1][:max_candidates]\n",
    "        coords_zyx = coords_zyx[sorted_indices]\n",
    "        candidate_probs = candidate_probs[sorted_indices]\n",
    "        \n",
    "    return coords_zyx, candidate_probs\n",
    "\n",
    "\n",
    "def extract_node_features(candidate_coords_zyx, base_model_outputs_list, patch_radius=2):\n",
    "    \"\"\"\n",
    "    Extracts features for GNN nodes from base model probability maps.\n",
    "    Args:\n",
    "        candidate_coords_zyx (np.ndarray): (N, 3) array of (z,y,x) node coordinates.\n",
    "        base_model_outputs_list (list of torch.Tensor): List of probability maps [(D,H,W), ...] from base models.\n",
    "                                                      Assumed to be on the same device.\n",
    "        patch_radius (int): Radius around candidate center to extract local stats (e.g., mean prob).\n",
    "    Returns:\n",
    "        torch.Tensor: Node features (N, num_features) on the same device as base_model_outputs.\n",
    "    \"\"\"\n",
    "    if candidate_coords_zyx.shape[0] == 0:\n",
    "        return torch.empty((0, len(base_model_outputs_list)), device=base_model_outputs_list[0].device if base_model_outputs_list else CONFIG[\"device\"])\n",
    "\n",
    "    num_nodes = candidate_coords_zyx.shape[0]\n",
    "    num_base_models = len(base_model_outputs_list)\n",
    "    # Feature: probability at center from each base model\n",
    "    # Feature: mean probability in a small patch around center from each base model\n",
    "    # num_features_per_model = 2\n",
    "    num_features_per_model = 1 # Just prob at center for simplicity now\n",
    "    \n",
    "    node_features = torch.zeros((num_nodes, num_base_models * num_features_per_model),\n",
    "                                device=base_model_outputs_list[0].device)\n",
    "\n",
    "    for i, (z, y, x) in enumerate(candidate_coords_zyx):\n",
    "        feat_idx = 0\n",
    "        for model_idx, prob_map in enumerate(base_model_outputs_list):\n",
    "            # Ensure prob_map is (D,H,W)\n",
    "            if prob_map.ndim == 4 and prob_map.shape[0] == 1:\n",
    "                prob_map = prob_map.squeeze(0)\n",
    "            \n",
    "            # Boundary checks for safety\n",
    "            d, h, w = prob_map.shape\n",
    "            z, y, x = int(z), int(y), int(x)\n",
    "            if not (0 <= z < d and 0 <= y < h and 0 <= x < w):\n",
    "                # Node is out of bounds for this prob_map (should not happen if maps cover same space)\n",
    "                # Fill with zeros or a specific value\n",
    "                node_features[i, feat_idx : feat_idx + num_features_per_model] = 0\n",
    "                feat_idx += num_features_per_model\n",
    "                continue\n",
    "\n",
    "            # Prob at center\n",
    "            node_features[i, feat_idx] = prob_map[z, y, x]\n",
    "            feat_idx += 1\n",
    "            \n",
    "            # Mean prob in patch (optional, adds complexity)\n",
    "            # z_min, z_max = max(0, z - patch_radius), min(d, z + patch_radius + 1)\n",
    "            # y_min, y_max = max(0, y - patch_radius), min(h, y + patch_radius + 1)\n",
    "            # x_min, x_max = max(0, x - patch_radius), min(w, x + patch_radius + 1)\n",
    "            # local_patch = prob_map[z_min:z_max, y_min:y_max, x_min:x_max]\n",
    "            # node_features[i, feat_idx] = local_patch.mean() if local_patch.numel() > 0 else 0\n",
    "            # feat_idx +=1\n",
    "\n",
    "    return node_features\n",
    "\n",
    "\n",
    "def create_graph_edges(candidate_coords_zyx, max_dist):\n",
    "    \"\"\"\n",
    "    Creates graph edges based on spatial proximity.\n",
    "    Args:\n",
    "        candidate_coords_zyx (np.ndarray): (N, 3) node coordinates (z,y,x).\n",
    "        max_dist (float): Maximum Euclidean distance to connect two nodes.\n",
    "    Returns:\n",
    "        torch.Tensor: Edge index (2, num_edges) in PyG format.\n",
    "    \"\"\"\n",
    "    if candidate_coords_zyx.shape[0] < 2:\n",
    "        return torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    dist_matrix = cdist(candidate_coords_zyx, candidate_coords_zyx)\n",
    "    adj_matrix = (dist_matrix > 0) & (dist_matrix <= max_dist) # Exclude self-loops for now, ensure >0\n",
    "    \n",
    "    edge_index_np = np.array(np.where(adj_matrix))\n",
    "    return torch.from_numpy(edge_index_np).long()\n",
    "\n",
    "\n",
    "def label_graph_nodes(candidate_coords_zyx, gt_motor_coords_zyx, matching_radius):\n",
    "    \"\"\"\n",
    "    Assigns ground truth labels (motor/non-motor) to GNN nodes.\n",
    "    Args:\n",
    "        candidate_coords_zyx (np.ndarray): (N, 3) node coordinates (z,y,x).\n",
    "        gt_motor_coords_zyx (np.ndarray): (M, 3) ground truth motor coordinates (z,y,x).\n",
    "        matching_radius (float): Max distance for a candidate to be matched to a GT motor.\n",
    "    Returns:\n",
    "        torch.Tensor: Node labels (N,), 1 for motor, 0 for non-motor.\n",
    "    \"\"\"\n",
    "    num_nodes = candidate_coords_zyx.shape[0]\n",
    "    node_labels = torch.zeros(num_nodes, dtype=torch.float) # Use float for BCEWithLogitsLoss\n",
    "\n",
    "    if num_nodes == 0 or gt_motor_coords_zyx.shape[0] == 0:\n",
    "        return node_labels\n",
    "\n",
    "    dist_matrix = cdist(candidate_coords_zyx, gt_motor_coords_zyx)\n",
    "    \n",
    "    # For each GT motor, find the closest candidate node within matching_radius\n",
    "    # This is a simple assignment. More complex matching (e.g. Hungarian) could be used.\n",
    "    # Here, any node close enough to ANY GT motor is labeled positive.\n",
    "    min_dist_to_gt = dist_matrix.min(axis=1)\n",
    "    matched_nodes_indices = np.where(min_dist_to_gt <= matching_radius)[0]\n",
    "    \n",
    "    node_labels[matched_nodes_indices] = 1.0\n",
    "    \n",
    "    return node_labels\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.2. GNN Model Definition (e.g., GAT)\n",
    "\n",
    "# %%\n",
    "if PYG_AVAILABLE:\n",
    "    class GATNet(torch.nn.Module):\n",
    "        def __init__(self, in_channels, hidden_channels, out_channels, num_layers, heads=4, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.dropout_p = dropout\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "            self.batch_norms = torch.nn.ModuleList() # Optional: Batch norm for GNNs\n",
    "\n",
    "            # Input layer\n",
    "            self.convs.append(pyg_nn.GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
    "            self.batch_norms.append(pyg_nn.BatchNorm(hidden_channels * heads))\n",
    "\n",
    "            # Hidden layers\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.convs.append(pyg_nn.GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
    "                self.batch_norms.append(pyg_nn.BatchNorm(hidden_channels * heads))\n",
    "            \n",
    "            # Output layer\n",
    "            self.convs.append(pyg_nn.GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "            # No batch norm for output typically, as it's logits\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            for i, conv in enumerate(self.convs[:-1]):\n",
    "                x = conv(x, edge_index)\n",
    "                x = self.batch_norms[i](x)\n",
    "                x = F.elu(x) # ELU or LeakyReLU are common with GAT\n",
    "                x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "            \n",
    "            x = self.convs[-1](x, edge_index) # Output logits for classification\n",
    "            return x # Output shape (num_nodes, out_channels)\n",
    "else:\n",
    "    class GATNet(torch.nn.Module): # Placeholder if PyG not available\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__()\n",
    "            self.dummy_layer = torch.nn.Linear(1,1)\n",
    "            if not PYG_AVAILABLE:\n",
    "              print(\"WARNING: GATNet is a placeholder as PyTorch Geometric is not installed.\")\n",
    "        def forward(self, x, edge_index):\n",
    "            if not PYG_AVAILABLE:\n",
    "                # Return dummy output of correct expected shape (num_nodes, out_channels=1)\n",
    "                # This won't train meaningfully but allows pipeline to run.\n",
    "                return torch.zeros((x.shape[0], 1), device=x.device)\n",
    "            return self.dummy_layer(torch.mean(x, dim=1, keepdim=True))\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.3. Generating Training Data for GNN\n",
    "# This involves running the trained base model(s) on a GNN training split of tomograms.\n",
    "\n",
    "# %%\n",
    "def generate_gnn_training_data(base_model, tomo_ids_for_gnn, data_root, gt_csv_path, gnn_config, base_model_config, device):\n",
    "    print(f\"Generating GNN training data for {len(tomo_ids_for_gnn)} tomograms...\")\n",
    "    base_model.eval()\n",
    "    base_model.to(device)\n",
    "    \n",
    "    gnn_data_list = []\n",
    "    \n",
    "    # Use validation transforms for loading full volumes for GNN data generation\n",
    "    # No random cropping here.\n",
    "    gnn_data_gen_transforms = Compose([\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ])\n",
    "\n",
    "    # Dataset for GNN data generation (full volumes)\n",
    "    gnn_raw_ds = CryoETDataset(\n",
    "        data_root=data_root,\n",
    "        tomo_ids=tomo_ids_for_gnn,\n",
    "        gt_csv_path=gt_csv_path,\n",
    "        transforms=gnn_data_gen_transforms,\n",
    "        target_radius=base_model_config[\"base_model_target_radius\"], # Not strictly needed for GNN data gen, but part of dataset\n",
    "        is_train=False\n",
    "    )\n",
    "    gnn_raw_loader = MonaiDataLoader(gnn_raw_ds, batch_size=1, shuffle=False, num_workers=base_model_config[\"num_workers\"])\n",
    "\n",
    "    full_gt_df = pd.read_csv(gt_csv_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(gnn_raw_loader, desc=\"Processing Tomos for GNN Data\"):\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            tomo_id = batch_data[\"id\"][0] # DataLoader with batch_size=1 gives list\n",
    "\n",
    "            # 1. Get base model probability map for the full tomogram\n",
    "            # Sliding window inference for full volume\n",
    "            prob_map = sliding_window_inference(\n",
    "                inputs,\n",
    "                roi_size=base_model_config[\"inference_roi_size_base\"],\n",
    "                sw_batch_size=base_model_config[\"inference_sw_batch_size_base\"],\n",
    "                predictor=base_model,\n",
    "                overlap=base_model_config[\"inference_overlap_base\"],\n",
    "                mode=\"gaussian\",\n",
    "                progress=False # Reduce verbosity\n",
    "            )\n",
    "            prob_map = torch.sigmoid(prob_map) # Apply sigmoid to get probabilities\n",
    "            # prob_map is (1, C, D, H, W), C=1. Squeeze to (D,H,W) for CPU ops.\n",
    "            prob_map_squeezed = prob_map.squeeze(0).squeeze(0) # Now (D,H,W)\n",
    "\n",
    "            # 2. Get candidate nodes\n",
    "            candidate_coords_zyx, candidate_probs = get_candidate_nodes_from_prob_map(\n",
    "                prob_map_squeezed,\n",
    "                gnn_config[\"gnn_node_candidate_threshold\"],\n",
    "                gnn_config[\"gnn_node_NMS_footprint\"]\n",
    "            )\n",
    "            if candidate_coords_zyx.shape[0] == 0:\n",
    "                print(f\"No candidate nodes found for tomogram {tomo_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 3. Extract node features\n",
    "            # For now, assuming one base model. If multiple, base_model_outputs_list would contain their maps.\n",
    "            node_features = extract_node_features(candidate_coords_zyx, [prob_map_squeezed])\n",
    "            \n",
    "            # 4. Create graph edges\n",
    "            edge_index = create_graph_edges(candidate_coords_zyx, gnn_config[\"gnn_edge_max_distance\"])\n",
    "\n",
    "            # 5. Label graph nodes\n",
    "            tomo_gt_df = full_gt_df[full_gt_df[\"tomo_id\"] == tomo_id]\n",
    "            if not tomo_gt_df.empty:\n",
    "                gt_motor_coords_zyx_np = tomo_gt_df[[\"center_z\", \"center_y\", \"center_x\"]].values.astype(int)\n",
    "            else:\n",
    "                gt_motor_coords_zyx_np = np.empty((0,3), dtype=int)\n",
    "            \n",
    "            # Matching radius for GNN node labeling (can be a hyperparameter)\n",
    "            # Should be related to base_model_target_radius.\n",
    "            gnn_node_labeling_radius = base_model_config[\"base_model_target_radius\"] + 2 \n",
    "            \n",
    "            node_labels = label_graph_nodes(candidate_coords_zyx, gt_motor_coords_zyx_np, gnn_node_labeling_radius)\n",
    "\n",
    "            # Create PyG Data object\n",
    "            if PYG_AVAILABLE:\n",
    "                graph_data = PyGData(\n",
    "                    x=node_features.to(device), # Keep features on device if GNN training on GPU\n",
    "                    edge_index=edge_index.to(device),\n",
    "                    y=node_labels.unsqueeze(1).to(device), # Target shape (num_nodes, 1)\n",
    "                    pos=torch.from_numpy(candidate_coords_zyx).float().to(device) # Store positions for potential visualization or use\n",
    "                )\n",
    "                graph_data.tomo_id = tomo_id # Store for reference\n",
    "                gnn_data_list.append(graph_data)\n",
    "            else: # Fallback if PyG not available (store as dict for potential later conversion)\n",
    "                 gnn_data_list.append({\n",
    "                     \"x\": node_features, \"edge_index\": edge_index, \"y\": node_labels.unsqueeze(1),\n",
    "                     \"pos\": torch.from_numpy(candidate_coords_zyx).float(), \"tomo_id\": tomo_id\n",
    "                 })\n",
    "\n",
    "\n",
    "    print(f\"Generated {len(gnn_data_list)} graphs for GNN training.\")\n",
    "    if not PYG_AVAILABLE and gnn_data_list:\n",
    "        print(\"WARNING: PyG not available. GNN data stored as dicts, actual GNN training will fail or be dummied.\")\n",
    "    return gnn_data_list\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.4. GNN Training Loop\n",
    "\n",
    "# %%\n",
    "def train_gnn_model(gnn_model, gnn_train_loader, gnn_val_loader, gnn_config, device, trial=None):\n",
    "    if not PYG_AVAILABLE:\n",
    "        print(\"Skipping GNN training as PyTorch Geometric is not available.\")\n",
    "        # Return a dummy model and results if in Optuna trial to avoid crash\n",
    "        # The dummy GATNet will handle forward pass by returning zeros.\n",
    "        # Create a dummy optimizer and loss for it to \"run\"\n",
    "        dummy_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=1e-3) # Won't do anything useful\n",
    "        return gnn_model, {\"train_losses\": [0], \"val_f2_scores\": [0], \"best_val_f2\": 0}\n",
    "\n",
    "\n",
    "    gnn_model.to(device)\n",
    "    optimizer = torch.optim.AdamW(gnn_model.parameters(), lr=gnn_config[\"gnn_lr\"], weight_decay=1e-5)\n",
    "    # Loss for node classification (binary: motor vs non-motor)\n",
    "    # BCEWithLogitsLoss is suitable as GNN outputs logits.\n",
    "    # Handle class imbalance for GNN nodes if necessary (many candidates are non-motors)\n",
    "    # Can use pos_weight in BCEWithLogitsLoss. Calculate from training data.\n",
    "    # For now, simple BCE.\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_f2 = -1\n",
    "    train_losses = []\n",
    "    val_f2_scores = [] # We'll use F-beta for GNN validation too\n",
    "\n",
    "    print(f\"Starting GNN training for {gnn_config['gnn_train_epochs']} epochs.\")\n",
    "\n",
    "    for epoch in range(gnn_config[\"gnn_train_epochs\"]):\n",
    "        gnn_model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        if not gnn_train_loader:\n",
    "            print(\"Skipping GNN training epoch as gnn_train_loader is None.\")\n",
    "            break\n",
    "\n",
    "        for graph_batch in tqdm(gnn_train_loader, desc=f\"GNN Epoch {epoch+1} Training\"):\n",
    "            # graph_batch is a Batch object from PyG if PyGDataLoader is used\n",
    "            # It automatically handles batching of graphs.\n",
    "            graph_batch = graph_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Ensure graph_batch.x and graph_batch.edge_index are correctly formatted\n",
    "            if not hasattr(graph_batch, 'x') or not hasattr(graph_batch, 'edge_index') or not hasattr(graph_batch, 'y'):\n",
    "                 print(f\"Skipping batch due to missing attributes: {graph_batch}\")\n",
    "                 continue\n",
    "            if graph_batch.x is None or graph_batch.edge_index is None or graph_batch.y is None:\n",
    "                 print(f\"Skipping batch due to None attributes: x:{graph_batch.x is None}, edge_index:{graph_batch.edge_index is None}, y:{graph_batch.y is None}\")\n",
    "                 continue\n",
    "            if graph_batch.num_nodes == 0 or graph_batch.num_edges == 0 and graph_batch.num_nodes > 1 : # Allow single node graph with no edges\n",
    "                # print(f\"Skipping batch with {graph_batch.num_nodes} nodes and {graph_batch.num_edges} edges.\")\n",
    "                # Instead of skipping, let GNN handle it; it might predict based on node features alone.\n",
    "                # Or, ensure graph generation avoids empty graphs if they cause issues.\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                out_logits = gnn_model(graph_batch.x, graph_batch.edge_index)\n",
    "                loss = criterion(out_logits, graph_batch.y) # y should be (num_nodes_in_batch, 1) float\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Runtime error during GNN training: {e}\")\n",
    "                print(f\"Graph batch details: Nodes: {graph_batch.num_nodes}, Edges: {graph_batch.num_edges}\")\n",
    "                print(f\"Node features shape: {graph_batch.x.shape if graph_batch.x is not None else 'None'}\")\n",
    "                print(f\"Edge index shape: {graph_batch.edge_index.shape if graph_batch.edge_index is not None else 'None'}\")\n",
    "                # Consider skipping problematic batch or re-raising\n",
    "                continue # Skip this batch\n",
    "\n",
    "\n",
    "        if num_batches > 0:\n",
    "            epoch_loss /= num_batches\n",
    "            train_losses.append(epoch_loss)\n",
    "            print(f\"GNN Epoch {epoch+1} average training loss: {epoch_loss:.4f}\")\n",
    "        else:\n",
    "            train_losses.append(0) # Or handle as error\n",
    "            print(f\"GNN Epoch {epoch+1} - No batches processed in training.\")\n",
    "\n",
    "\n",
    "        # GNN Validation (on a GNN validation set, could be same as base model val set)\n",
    "        if gnn_val_loader:\n",
    "            gnn_model.eval()\n",
    "            all_preds = []\n",
    "            all_true_labels = []\n",
    "            with torch.no_grad():\n",
    "                for graph_batch_val in tqdm(gnn_val_loader, desc=f\"GNN Epoch {epoch+1} Validation\"):\n",
    "                    graph_batch_val = graph_batch_val.to(device)\n",
    "                    if not hasattr(graph_batch_val, 'x') or not hasattr(graph_batch_val, 'edge_index') or not hasattr(graph_batch_val, 'y'):\n",
    "                        continue\n",
    "                    if graph_batch_val.x is None or graph_batch_val.edge_index is None or graph_batch_val.y is None:\n",
    "                        continue\n",
    "                    \n",
    "                    out_logits_val = gnn_model(graph_batch_val.x, graph_batch_val.edge_index)\n",
    "                    preds_probs_val = torch.sigmoid(out_logits_val).cpu().numpy().ravel()\n",
    "                    true_labels_val = graph_batch_val.y.cpu().numpy().ravel()\n",
    "                    \n",
    "                    all_preds.extend(preds_probs_val)\n",
    "                    all_true_labels.extend(true_labels_val)\n",
    "            \n",
    "            if all_true_labels: # If any validation data processed\n",
    "                # For F-beta, we need TP, FP, FN. Threshold predictions.\n",
    "                # A common threshold is 0.5, but this could also be optimized.\n",
    "                threshold = 0.5\n",
    "                preds_binary = (np.array(all_preds) >= threshold).astype(int)\n",
    "                true_binary = (np.array(all_true_labels) >= 0.5).astype(int) # GT labels are 0 or 1\n",
    "\n",
    "                f2_score_val, _, _ = calculate_fbeta_precision_recall(preds_binary, true_binary, beta=2.0)\n",
    "                val_f2_scores.append(f2_score_val)\n",
    "                print(f\"GNN Epoch {epoch+1} validation F2-score: {f2_score_val:.4f}\")\n",
    "\n",
    "                if f2_score_val > best_val_f2:\n",
    "                    best_val_f2 = f2_score_val\n",
    "                    # Save best GNN model\n",
    "                    gnn_save_path = Path(CONFIG[\"output_dir\"]) / f\"{CONFIG['gnn_model_name']}_best_f2_{best_val_f2:.4f}.pth\"\n",
    "                    torch.save(gnn_model.state_dict(), gnn_save_path)\n",
    "                    print(f\"Saved new best GNN model to {gnn_save_path}\")\n",
    "                \n",
    "                if trial:\n",
    "                    trial.report(f2_score_val, epoch)\n",
    "                    if trial.should_prune():\n",
    "                        torch.cuda.empty_cache()\n",
    "                        raise optuna.exceptions.TrialPruned()\n",
    "            else:\n",
    "                print(\"GNN Epoch {epoch+1} - No validation data processed or no true labels.\")\n",
    "                if trial: # Report train loss if no val metric\n",
    "                    trial.report(epoch_loss if num_batches > 0 else 0.0 , epoch) # Or some default low val score\n",
    "                    if trial.should_prune():\n",
    "                         torch.cuda.empty_cache()\n",
    "                         raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "\n",
    "    print(f\"Finished GNN training. Best validation F2-score: {best_val_f2:.4f}\")\n",
    "    # Load best GNN model\n",
    "    if best_val_f2 != -1:\n",
    "        best_gnn_path_str = f\"{CONFIG['gnn_model_name']}_best_f2_{best_val_f2:.4f}.pth\"\n",
    "        best_gnn_path = Path(CONFIG[\"output_dir\"]) / best_gnn_path_str\n",
    "        if best_gnn_path.exists():\n",
    "            gnn_model.load_state_dict(torch.load(best_gnn_path, map_location=device))\n",
    "            print(f\"Loaded best GNN model from {best_gnn_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Best GNN model path {best_gnn_path} not found. Using last epoch GNN model.\")\n",
    "\n",
    "\n",
    "    return gnn_model, {\"train_losses\": train_losses, \"val_f2_scores\": val_f2_scores, \"best_val_f2\": best_val_f2}\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 5: Training Orchestration (Base Model + GNN)\n",
    "# This function will manage the two-stage training process.\n",
    "\n",
    "# %%\n",
    "def run_full_training_pipeline(config, trial=None):\n",
    "    \"\"\"\n",
    "    Manages the entire training:\n",
    "    1. Train base model(s).\n",
    "    2. Generate GNN training data using trained base model(s).\n",
    "    3. Train GNN meta-model.\n",
    "    Returns the trained GNN model and the last trained base model.\n",
    "    \"\"\"\n",
    "    # --- 1. Train Base Model ---\n",
    "    print(\"--- Stage 1: Training Base Model ---\")\n",
    "    base_model = get_base_model(config).to(config[\"device\"])\n",
    "    \n",
    "    # For Optuna, if base model HPs are tuned, they should be passed via config.\n",
    "    # For now, use fixed HPs from global CONFIG, or let Optuna override them in `objective`\n",
    "    \n",
    "    # Ensure data loaders are available\n",
    "    if not train_loader_base:\n",
    "        print(\"Cannot train base model: train_loader_base is None.\")\n",
    "        # If in Optuna trial, this is a failure or needs specific handling\n",
    "        if trial: raise optuna.exceptions.TrialPruned(\"No training data for base model\")\n",
    "        return None, None, None # Or some indicator of failure\n",
    "\n",
    "    trained_base_model, base_model_history = train_base_model(\n",
    "        base_model, train_loader_base, val_loader_base, config, trial=trial # Pass trial for pruning base model\n",
    "    )\n",
    "    # Note: if base model training prunes, this function will exit via exception.\n",
    "\n",
    "    # --- 2. Generate GNN Training Data ---\n",
    "    print(\"--- Stage 2: Generating GNN Training Data ---\")\n",
    "    # Use validation tomograms for GNN training/validation to avoid leakage from base model's training set.\n",
    "    # Split VAL_TOMO_IDS further if needed: e.g., 70% for GNN train, 30% for GNN val.\n",
    "    # For simplicity here, use all VAL_TOMO_IDS for GNN data generation.\n",
    "    # This data will then be split into GNN_train_graphs and GNN_val_graphs.\n",
    "    if not VAL_TOMO_IDS:\n",
    "        print(\"Cannot generate GNN data: VAL_TOMO_IDS is empty.\")\n",
    "        if trial: raise optuna.exceptions.TrialPruned(\"No validation data for GNN generation\")\n",
    "        return trained_base_model, None, None\n",
    "\n",
    "    gnn_graphs_all = generate_gnn_training_data(\n",
    "        trained_base_model, VAL_TOMO_IDS, # Use validation set of tomograms\n",
    "        config[\"dataset_root\"], config[\"gt_csv_path\"], config, config, config[\"device\"]\n",
    "    )\n",
    "\n",
    "    if not gnn_graphs_all:\n",
    "        print(\"No graphs generated for GNN training. Cannot train GNN.\")\n",
    "        if trial: raise optuna.exceptions.TrialPruned(\"No GNN graphs generated\")\n",
    "        return trained_base_model, None, None # Return trained base model, but no GNN\n",
    "\n",
    "    # Split GNN graphs into train/val for GNN training\n",
    "    random.shuffle(gnn_graphs_all) # Shuffle before splitting\n",
    "    gnn_train_split_idx = int(0.8 * len(gnn_graphs_all)) # 80/20 split\n",
    "    gnn_train_graphs = gnn_graphs_all[:gnn_train_split_idx]\n",
    "    gnn_val_graphs = gnn_graphs_all[gnn_train_split_idx:]\n",
    "\n",
    "    if not gnn_train_graphs:\n",
    "        print(\"Not enough GNN graphs for training split. GNN training skipped.\")\n",
    "        if trial: trial.report(0.0, config[\"base_model_train_epochs\"]) # Report 0 for final GNN metric\n",
    "        # No Optuna prune here, as base model trained. The pipeline just didn't get to GNN.\n",
    "        return trained_base_model, None, None\n",
    "\n",
    "\n",
    "    if PYG_AVAILABLE:\n",
    "        gnn_train_loader = PyGDataLoader(gnn_train_graphs, batch_size=config[\"gnn_batch_size\"], shuffle=True)\n",
    "        gnn_val_loader = PyGDataLoader(gnn_val_graphs, batch_size=config[\"gnn_batch_size\"], shuffle=False) if gnn_val_graphs else None\n",
    "    else: # Fallback if PyG not available\n",
    "        # Cannot create PyGDataLoaders. Training will be dummied.\n",
    "        print(\"PyG not available. GNN DataLoaders are placeholders.\")\n",
    "        gnn_train_loader = gnn_train_graphs # Pass list of dicts\n",
    "        gnn_val_loader = gnn_val_graphs if gnn_val_graphs else None\n",
    "\n",
    "\n",
    "    # --- 3. Train GNN Meta-Model ---\n",
    "    print(\"--- Stage 3: Training GNN Meta-Model ---\")\n",
    "    # Determine GNN input channels from features.\n",
    "    # Example: if node_features are (N, num_base_models * 1), then in_channels is num_base_models.\n",
    "    # From extract_node_features: num_features_per_model = 1. So in_channels = num_base_models.\n",
    "    # If PYG_AVAILABLE and gnn_train_graphs:\n",
    "    #    gnn_in_channels = gnn_train_graphs[0].x.shape[1] if gnn_train_graphs[0].x is not None else config[\"num_base_models\"]\n",
    "    # elif gnn_train_graphs: # list of dicts\n",
    "    #    gnn_in_channels = gnn_train_graphs[0]['x'].shape[1] if gnn_train_graphs[0]['x'] is not None else config[\"num_base_models\"]\n",
    "    # else: # Default if no data\n",
    "    #    gnn_in_channels = config[\"num_base_models\"]\n",
    "    # Safer way, check first element of loader if it exists\n",
    "    if gnn_train_loader and PYG_AVAILABLE:\n",
    "        first_gnn_batch_for_shape = first(gnn_train_loader)\n",
    "        if first_gnn_batch_for_shape and hasattr(first_gnn_batch_for_shape, 'x') and first_gnn_batch_for_shape.x is not None:\n",
    "            gnn_in_channels = first_gnn_batch_for_shape.x.shape[1]\n",
    "        else: # Fallback if batch is weird or empty\n",
    "            gnn_in_channels = config[\"num_base_models\"] # Default\n",
    "    elif gnn_train_graphs and not PYG_AVAILABLE: # list of dicts\n",
    "        gnn_in_channels = gnn_train_graphs[0]['x'].shape[1] if gnn_train_graphs[0]['x'] is not None else config[\"num_base_models\"]\n",
    "    else:\n",
    "        gnn_in_channels = config[\"num_base_models\"]\n",
    "\n",
    "\n",
    "    gnn_model = GATNet(\n",
    "        in_channels=gnn_in_channels,\n",
    "        hidden_channels=config[\"gnn_hidden_channels\"],\n",
    "        out_channels=1, # Binary classification (motor/non-motor)\n",
    "        num_layers=config[\"gnn_num_layers\"],\n",
    "        heads=config[\"gnn_gat_heads\"]\n",
    "    ).to(config[\"device\"])\n",
    "\n",
    "    trained_gnn_model, gnn_model_history = train_gnn_model(\n",
    "        gnn_model, gnn_train_loader, gnn_val_loader, config, config[\"device\"], trial=trial # Pass trial for GNN pruning\n",
    "    )\n",
    "\n",
    "    # The metric for Optuna should be from evaluating the *entire pipeline* on a final hold-out validation set.\n",
    "    # Here, gnn_model_history[\"best_val_f2\"] is on GNN's own validation split.\n",
    "    # This is what Optuna will use if called from objective function.\n",
    "    \n",
    "    # For returning from a standalone run:\n",
    "    pipeline_results = {\n",
    "        \"base_model_history\": base_model_history,\n",
    "        \"gnn_model_history\": gnn_model_history,\n",
    "        \"final_gnn_val_metric\": gnn_model_history.get(\"best_val_f2\", 0) if gnn_model_history else 0\n",
    "    }\n",
    "    \n",
    "    return trained_base_model, trained_gnn_model, pipeline_results\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 6: Evaluation Metrics and Utilities\n",
    "#\n",
    "# ### F-beta Score and Detection Matching\n",
    "\n",
    "# %%\n",
    "def calculate_fbeta_precision_recall(predictions_binary, ground_truth_binary, beta, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Calculates F-beta score, precision, and recall.\n",
    "    Args:\n",
    "        predictions_binary (np.ndarray or torch.Tensor): Binary predictions (0 or 1).\n",
    "        ground_truth_binary (np.ndarray or torch.Tensor): Binary ground truth (0 or 1).\n",
    "        beta (float): The beta value for F-beta score.\n",
    "        epsilon (float): Small value to prevent division by zero.\n",
    "    Returns:\n",
    "        tuple: (fbeta_score, precision, recall)\n",
    "    \"\"\"\n",
    "    if isinstance(predictions_binary, torch.Tensor):\n",
    "        predictions_binary = predictions_binary.cpu().numpy()\n",
    "    if isinstance(ground_truth_binary, torch.Tensor):\n",
    "        ground_truth_binary = ground_truth_binary.cpu().numpy()\n",
    "\n",
    "    TP = np.sum((predictions_binary == 1) & (ground_truth_binary == 1))\n",
    "    FP = np.sum((predictions_binary == 1) & (ground_truth_binary == 0))\n",
    "    FN = np.sum((predictions_binary == 0) & (ground_truth_binary == 1))\n",
    "\n",
    "    precision = TP / (TP + FP + epsilon)\n",
    "    recall = TP / (TP + FN + epsilon)\n",
    "\n",
    "    fbeta_denominator = (beta**2 * precision) + recall\n",
    "    if fbeta_denominator > epsilon:\n",
    "        fbeta_score = (1 + beta**2) * (precision * recall) / fbeta_denominator\n",
    "    else:\n",
    "        fbeta_score = 0.0\n",
    "        \n",
    "    return fbeta_score, precision, recall\n",
    "\n",
    "def match_detections_3d(pred_centers_scores, gt_centers, matching_dist_threshold):\n",
    "    \"\"\"\n",
    "    Matches predicted 3D detections to ground truth 3D centers.\n",
    "    Args:\n",
    "        pred_centers_scores (list of tuples): [( (x,y,z), score ), ...]. Assumes x,y,z order.\n",
    "        gt_centers (np.ndarray): Array of GT (x,y,z) centers, shape (M, 3).\n",
    "        matching_dist_threshold (float): Maximum distance for a match.\n",
    "    Returns:\n",
    "        tuple: (TP, FP, FN, matched_pairs_info)\n",
    "        matched_pairs_info: list of (pred_idx, gt_idx, dist, score)\n",
    "    \"\"\"\n",
    "    if not pred_centers_scores or gt_centers.shape[0] == 0:\n",
    "        FP = len(pred_centers_scores)\n",
    "        FN = gt_centers.shape[0]\n",
    "        return 0, FP, FN, []\n",
    "\n",
    "    pred_centers_np = np.array([p[0] for p in pred_centers_scores])\n",
    "    pred_scores_np = np.array([p[1] for p in pred_centers_scores])\n",
    "\n",
    "    num_preds = pred_centers_np.shape[0]\n",
    "    num_gts = gt_centers.shape[0]\n",
    "\n",
    "    if num_preds == 0:\n",
    "        return 0, 0, num_gts, []\n",
    "\n",
    "    dist_matrix = cdist(pred_centers_np, gt_centers) # (num_preds, num_gts)\n",
    "\n",
    "    # Greedy matching based on highest score first (or could use Hungarian algorithm for optimal assignment)\n",
    "    # For simplicity, iterate through predictions sorted by score (descending)\n",
    "    \n",
    "    # Create a list of (original_pred_idx, score) to sort\n",
    "    sorted_pred_indices = np.argsort(pred_scores_np)[::-1] # Sort by score, descending\n",
    "\n",
    "    gt_matched_flags = np.zeros(num_gts, dtype=bool)\n",
    "    pred_is_tp = np.zeros(num_preds, dtype=bool)\n",
    "    matched_pairs_info = []\n",
    "\n",
    "    for pred_idx_sorted in sorted_pred_indices:\n",
    "        # Find GTs within threshold for this prediction\n",
    "        dists_to_gts_for_this_pred = dist_matrix[pred_idx_sorted, :]\n",
    "        possible_gt_matches_indices = np.where((dists_to_gts_for_this_pred <= matching_dist_threshold) & (~gt_matched_flags))[0]\n",
    "\n",
    "        if len(possible_gt_matches_indices) > 0:\n",
    "            # Match to the closest available GT\n",
    "            best_gt_match_local_idx = np.argmin(dists_to_gts_for_this_pred[possible_gt_matches_indices])\n",
    "            gt_idx_global = possible_gt_matches_indices[best_gt_match_local_idx]\n",
    "            \n",
    "            pred_is_tp[pred_idx_sorted] = True\n",
    "            gt_matched_flags[gt_idx_global] = True\n",
    "            matched_pairs_info.append((\n",
    "                pred_idx_sorted, \n",
    "                gt_idx_global, \n",
    "                dists_to_gts_for_this_pred[gt_idx_global], \n",
    "                pred_scores_np[pred_idx_sorted]\n",
    "            ))\n",
    "\n",
    "    TP = np.sum(pred_is_tp)\n",
    "    FP = num_preds - TP\n",
    "    FN = num_gts - np.sum(gt_matched_flags)\n",
    "            \n",
    "    return TP, FP, FN, matched_pairs_info\n",
    "\n",
    "\n",
    "def evaluate_pipeline_on_tomogram(tomo_id, pred_detections, gt_csv_path, matching_dist_thresh_3d, beta=2.0):\n",
    "    \"\"\"\n",
    "    Evaluates detections for a single 3D tomogram.\n",
    "    Args:\n",
    "        tomo_id (str): The ID of the tomogram.\n",
    "        pred_detections (list of dicts): [{'center':(x,y,z), 'score':score, 'bbox':(...)}].\n",
    "        gt_csv_path (str): Path to the ground truth CSV.\n",
    "        matching_dist_thresh_3d (float): Distance threshold for matching.\n",
    "        beta (float): Beta for F-beta score.\n",
    "    Returns:\n",
    "        dict: {'fbeta': score, 'precision': score, 'recall': score, 'tp': count, 'fp': count, 'fn': count}\n",
    "    \"\"\"\n",
    "    full_gt_df = pd.read_csv(gt_csv_path)\n",
    "    tomo_gt_df = full_gt_df[full_gt_df[\"tomo_id\"] == tomo_id]\n",
    "\n",
    "    if tomo_gt_df.empty:\n",
    "        gt_centers_xyz = np.empty((0,3))\n",
    "    else:\n",
    "        # Ensure correct X,Y,Z order for cdist\n",
    "        gt_centers_xyz = tomo_gt_df[[\"center_x\", \"center_y\", \"center_z\"]].values.astype(float)\n",
    "\n",
    "    pred_centers_scores = []\n",
    "    for det in pred_detections:\n",
    "        # Ensure center is (x,y,z) tuple/list\n",
    "        pred_centers_scores.append((tuple(det['center']), det['score']))\n",
    "    \n",
    "    TP, FP, FN, _ = match_detections_3d(pred_centers_scores, gt_centers_xyz, matching_dist_thresh_3d)\n",
    "    \n",
    "    fbeta, precision, recall = calculate_fbeta_precision_recall(\n",
    "        np.concatenate([np.ones(TP), np.ones(FP), np.zeros(FN)]), # Equivalent binary predictions for TP,FP\n",
    "        np.concatenate([np.ones(TP), np.zeros(FP), np.ones(FN)]), # Equivalent binary GT for TP,FN\n",
    "        beta=beta\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        f\"{CONFIG['optuna_objective_metric']}\": fbeta, # Use the metric name from config for consistency\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"tp\": TP,\n",
    "        \"fp\": FP,\n",
    "        \"fn\": FN\n",
    "    }\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 7: Inference Pipeline\n",
    "# Includes Mode 1 (Full 3D Tomograms) and Mode 2 (Individual 2D Slices adaptation).\n",
    "#\n",
    "# ### Helper: Post-process GNN outputs to detections\n",
    "\n",
    "# %%\n",
    "def gnn_output_to_detections(graph_data_with_preds, gnn_score_threshold, output_bbox_size_3d_or_2d):\n",
    "    \"\"\"\n",
    "    Converts GNN output probabilities on nodes to final detection list.\n",
    "    Args:\n",
    "        graph_data_with_preds (PyGData or dict): Graph data object containing:\n",
    "            - `pos`: (N,3) or (N,2) tensor of node coordinates (z,y,x) or (y,x).\n",
    "            - `pred_probs`: (N,) tensor of GNN predicted probabilities for nodes.\n",
    "            - (optional) `tomo_id` or `slice_id`\n",
    "        gnn_score_threshold (float): Confidence threshold to make a detection.\n",
    "        output_bbox_size_3d_or_2d (tuple): Size of bbox (Dx,Dy,Dz) or (Dy,Dx). Assumed odd for centering.\n",
    "    Returns:\n",
    "        list of dicts: [{'center':(x,y,z) or (x,y), 'bbox':(...), 'score':score, 'id': id}]\n",
    "    \"\"\"\n",
    "    detections = []\n",
    "    node_coords = graph_data_with_preds.pos.cpu().numpy() # (z,y,x) or (y,x)\n",
    "    node_probs = graph_data_with_preds.pred_probs.cpu().numpy()\n",
    "\n",
    "    is_3d = (len(output_bbox_size_3d_or_2d) == 3)\n",
    "\n",
    "    for i in range(node_coords.shape[0]):\n",
    "        score = node_probs[i]\n",
    "        if score >= gnn_score_threshold:\n",
    "            if is_3d:\n",
    "                # node_coords are (z,y,x)\n",
    "                center_z, center_y, center_x = node_coords[i, 0], node_coords[i, 1], node_coords[i, 2]\n",
    "                # Output center as (x,y,z)\n",
    "                center_out = (float(center_x), float(center_y), float(center_z))\n",
    "                \n",
    "                # Bbox (X_min,Y_min,Z_min,X_max,Y_max,Z_max)\n",
    "                # Size is (size_x, size_y, size_z) if output_bbox_size is given that way (Dx,Dy,Dz)\n",
    "                # Let's assume output_bbox_size is (SZ, SY, SX) to match ZYX coords.\n",
    "                # Or, consistently use (X,Y,Z) for bbox size. Let's use X,Y,Z for bbox size.\n",
    "                bs_x, bs_y, bs_z = output_bbox_size_3d_or_2d[0], output_bbox_size_3d_or_2d[1], output_bbox_size_3d_or_2d[2]\n",
    "                \n",
    "                bbox = (\n",
    "                    center_x - (bs_x -1)//2, center_y - (bs_y -1)//2, center_z - (bs_z -1)//2,\n",
    "                    center_x + (bs_x -1)//2, center_y + (bs_y -1)//2, center_z + (bs_z -1)//2\n",
    "                )\n",
    "            else: # 2D\n",
    "                # node_coords are (y,x) for 2D\n",
    "                center_y, center_x = node_coords[i, 0], node_coords[i, 1]\n",
    "                # Output center as (x,y)\n",
    "                center_out = (float(center_x), float(center_y))\n",
    "\n",
    "                # Bbox (X_min,Y_min,X_max,Y_max)\n",
    "                bs_x, bs_y = output_bbox_size_3d_or_2d[0], output_bbox_size_3d_or_2d[1] # Assumed (size_x, size_y)\n",
    "                bbox = (\n",
    "                    center_x - (bs_x -1)//2, center_y - (bs_y -1)//2,\n",
    "                    center_x + (bs_x -1)//2, center_y + (bs_y -1)//2\n",
    "                )\n",
    "\n",
    "            detections.append({\n",
    "                \"center\": center_out,\n",
    "                \"bbox\": tuple(map(float, bbox)),\n",
    "                \"score\": float(score),\n",
    "                \"id\": f\"{getattr(graph_data_with_preds, 'id_prefix', 'det')}_{i}\"\n",
    "            })\n",
    "    return detections\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 7.1. Mode 1: Inference on Full 3D Tomograms\n",
    "\n",
    "# %%\n",
    "def inference_on_3d_tomogram(tomo_id, base_model, gnn_model, config, data_root, device):\n",
    "    \"\"\"\n",
    "    Performs full pipeline inference on a single 3D tomogram.\n",
    "    Returns: list of detection dictionaries.\n",
    "    \"\"\"\n",
    "    base_model.eval().to(device)\n",
    "    if gnn_model: gnn_model.eval().to(device)\n",
    "\n",
    "    # --- Load and preprocess tomogram ---\n",
    "    tomo_path = Path(data_root) / tomo_id\n",
    "    if not tomo_path.is_dir():\n",
    "        print(f\"Tomogram directory {tomo_path} not found for inference.\")\n",
    "        return []\n",
    "\n",
    "    img_arr, meta = JPGSequenceReader().read(str(tomo_path)) # H,W,D\n",
    "    img_arr = img_arr.transpose(2,0,1) # D,H,W\n",
    "    \n",
    "    data_item = {\"image\": img_arr, \"id\": tomo_id}\n",
    "    \n",
    "    # Use validation transforms but without label key\n",
    "    inference_transforms_3d = Compose([\n",
    "        EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        ToTensord(keys=[\"image\"]),\n",
    "    ])\n",
    "    processed_tomo = inference_transforms_3d(data_item)\n",
    "    input_tensor = processed_tomo[\"image\"].unsqueeze(0).to(device) # Add batch dim: (1,C,D,H,W)\n",
    "\n",
    "    # --- Base Model Inference ---\n",
    "    with torch.no_grad():\n",
    "        base_model_output = sliding_window_inference(\n",
    "            input_tensor,\n",
    "            roi_size=config[\"inference_roi_size_base\"],\n",
    "            sw_batch_size=config[\"inference_sw_batch_size_base\"],\n",
    "            predictor=base_model,\n",
    "            overlap=config[\"inference_overlap_base\"],\n",
    "            mode=\"gaussian\",\n",
    "            progress=True\n",
    "        )\n",
    "    base_prob_map = torch.sigmoid(base_model_output.squeeze(0).squeeze(0)) # (D,H,W) on device\n",
    "\n",
    "    if not gnn_model or not PYG_AVAILABLE: # If no GNN, use base model output directly\n",
    "        print(\"Performing inference using only base model (GNN not available or specified).\")\n",
    "        candidate_coords_zyx, candidate_probs = get_candidate_nodes_from_prob_map(\n",
    "            base_prob_map,\n",
    "            config[\"gnn_node_candidate_threshold\"], # Use GNN's candidate threshold as a proxy\n",
    "            config[\"gnn_node_NMS_footprint\"]\n",
    "        )\n",
    "        # Convert these directly to detections\n",
    "        detections = []\n",
    "        for i, (z,y,x) in enumerate(candidate_coords_zyx):\n",
    "            center_out = (float(x), float(y), float(z))\n",
    "            bs_x, bs_y, bs_z = config[\"output_bbox_size_3d\"]\n",
    "            bbox = (\n",
    "                x - (bs_x-1)//2, y - (bs_y-1)//2, z - (bs_z-1)//2,\n",
    "                x + (bs_x-1)//2, y + (bs_y-1)//2, z + (bs_z-1)//2\n",
    "            )\n",
    "            detections.append({\n",
    "                \"center\": center_out, \"bbox\": tuple(map(float, bbox)), \n",
    "                \"score\": float(candidate_probs[i]), \"id\": f\"{tomo_id}_base_det_{i}\"})\n",
    "        return detections\n",
    "\n",
    "\n",
    "    # --- Graph Construction for GNN ---\n",
    "    candidate_coords_zyx, _ = get_candidate_nodes_from_prob_map( # Don't need probs here, GNN re-evals\n",
    "        base_prob_map,\n",
    "        config[\"gnn_node_candidate_threshold\"],\n",
    "        config[\"gnn_node_NMS_footprint\"]\n",
    "    )\n",
    "    if candidate_coords_zyx.shape[0] == 0:\n",
    "        print(f\"No candidate nodes from base model for GNN input on tomo {tomo_id}.\")\n",
    "        return []\n",
    "\n",
    "    node_features = extract_node_features(candidate_coords_zyx, [base_prob_map]) # Pass list of maps\n",
    "    edge_index = create_graph_edges(candidate_coords_zyx, config[\"gnn_edge_max_distance\"])\n",
    "    \n",
    "    graph_for_gnn = PyGData(\n",
    "        x=node_features.to(device),\n",
    "        edge_index=edge_index.to(device),\n",
    "        pos=torch.from_numpy(candidate_coords_zyx).float().to(device)\n",
    "    )\n",
    "    graph_for_gnn.id_prefix = f\"{tomo_id}_gnn_det\"\n",
    "\n",
    "\n",
    "    # --- GNN Meta-Model Inference ---\n",
    "    with torch.no_grad():\n",
    "        gnn_logits = gnn_model(graph_for_gnn.x, graph_for_gnn.edge_index)\n",
    "        gnn_probs = torch.sigmoid(gnn_logits).squeeze(-1) # (Num_nodes,)\n",
    "    \n",
    "    graph_for_gnn.pred_probs = gnn_probs\n",
    "\n",
    "    # --- Post-process GNN outputs ---\n",
    "    # Threshold for GNN output can be different from candidate generation threshold\n",
    "    gnn_final_score_threshold = config.get(\"gnn_final_score_threshold\", 0.5) # Default 0.5 if not in config\n",
    "    \n",
    "    final_detections = gnn_output_to_detections(\n",
    "        graph_for_gnn,\n",
    "        gnn_final_score_threshold,\n",
    "        config[\"output_bbox_size_3d\"]\n",
    "    )\n",
    "    \n",
    "    return final_detections\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 7.2. Mode 2: Inference on Individual 2D Slices (Adaptation)\n",
    "# This is the critical adaptation part.\n",
    "#\n",
    "# **Conceptual Explanation for 2D Slice Adaptation:**\n",
    "#\n",
    "# 1.  **Base Model Adaptation:**\n",
    "#     *   A single 2D input slice `(H, W)` is received.\n",
    "#     *   To feed it to the 3D CNN base model, we create a \"pseudo-3D\" volume. A simple method is to replicate the slice `D_pseudo` times along the Z-axis, resulting in a `(D_pseudo, H, W)` volume. `D_pseudo` (e.g., `CONFIG[\"pseudo_3d_depth_for_2d_slice\"]`) should be chosen carefully:\n",
    "#         *   It should be large enough to contain meaningful features for the 3D CNN, ideally related to its receptive field or patch depth during training.\n",
    "#         *   If `D_pseudo` is smaller than the Z-dimension of `CONFIG[\"inference_roi_size_base\"]`, the sliding window Z-dimension needs to be adjusted or padding applied. For simplicity, we might make `D_pseudo` equal to `inference_roi_size_base[2]`.\n",
    "#     *   The base 3D CNN (using `sliding_window_inference` if `(H, W)` is larger than the XY dimensions of `inference_roi_size_base`) processes this pseudo-3D volume.\n",
    "#     *   The output is a pseudo-3D probability map, e.g., `(1, 1, D_pseudo, H, W)`.\n",
    "#     *   We extract the 2D probability map corresponding to the original input slice, typically the central slice of the `D_pseudo` dimension: `prob_map_2d = pseudo_3d_prob_map[:, :, D_pseudo // 2, :, :]`.\n",
    "#\n",
    "# 2.  **GNN Meta-Model Adaptation:**\n",
    "#     *   **Candidate Generation (2D):** From the `prob_map_2d`, extract 2D candidate motor locations (nodes) using 2D NMS. Node coordinates will be `(y, x)`.\n",
    "#     *   **Node Feature Extraction (2D):** For each 2D candidate node, features are extracted from `prob_map_2d`. These features should be analogous to those used in 3D GNN training (e.g., probability at the candidate center from the base model's (adapted) 2D output).\n",
    "#     *   **Graph Construction (2D):** A 2D graph is built. Edges connect nodes based on 2D spatial proximity on the slice.\n",
    "#     *   **GNN Inference:** The GNN model (which was trained on 3D graph data) processes this 2D graph.\n",
    "#         *   *Challenge & Assumption:* The GNN's architecture (especially if using GAT) should be somewhat robust to changes in graph dimensionality (implicitly, as node features are similar and graph structure is just adjacency). The number of node features must match what the GNN expects.\n",
    "#     *   **Output:** The GNN yields 2D detections (center `(x,y)` on slice, 2D bounding box, confidence score).\n",
    "#\n",
    "# **Challenges with 2D Adaptation:**\n",
    "# *   **Feature Consistency:** Ensuring node features derived from adapted 2D base model outputs are semantically similar enough to those from 3D outputs for the GNN to generalize.\n",
    "# *   **Graph Structure:** The GNN sees graphs with potentially different typical node degrees or densities. GATs are generally better at this than GCNs.\n",
    "# *   **Pseudo-3D Volume:** The choice of `D_pseudo` and replication strategy can impact base model performance. Replicating might create artificial Z-continuity. Padding with zeros or mean values are alternatives.\n",
    "# *   **Information Loss:** The GNN was trained on 3D contextual information. Applying it to 2D graphs means it loses Z-axis relational information between nodes. Performance might be lower than full 3D inference.\n",
    "\n",
    "# %%\n",
    "def inference_on_2d_slice(slice_img_hw, slice_id, base_model, gnn_model, config, device):\n",
    "    \"\"\"\n",
    "    Performs adapted full pipeline inference on a single 2D slice.\n",
    "    Args:\n",
    "        slice_img_hw (np.ndarray): Single 2D slice (H, W), grayscale.\n",
    "        slice_id (str): Identifier for the slice.\n",
    "    Returns: list of 2D detection dictionaries.\n",
    "    \"\"\"\n",
    "    base_model.eval().to(device)\n",
    "    if gnn_model: gnn_model.eval().to(device)\n",
    "\n",
    "    # --- 1. Base Model Adaptation: Pseudo-3D Volume Creation ---\n",
    "    h, w = slice_img_hw.shape\n",
    "    d_pseudo = config[\"pseudo_3d_depth_for_2d_slice\"] # e.g., 32 or 64, or even config[\"inference_roi_size_base\"][2]\n",
    "\n",
    "    # Replicate slice to form D_pseudo x H x W volume\n",
    "    pseudo_3d_volume_dhw = np.stack([slice_img_hw] * d_pseudo, axis=0) # (D_pseudo, H, W)\n",
    "    \n",
    "    data_item = {\"image\": pseudo_3d_volume_dhw, \"id\": slice_id}\n",
    "    \n",
    "    inference_transforms_pseudo3d = Compose([ # Similar to 3D inference transforms\n",
    "        EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"), # -> (1, D_pseudo, H, W)\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        ToTensord(keys=[\"image\"]),\n",
    "    ])\n",
    "    processed_pseudo_3d = inference_transforms_pseudo3d(data_item)\n",
    "    input_tensor_pseudo_3d = processed_pseudo_3d[\"image\"].unsqueeze(0).to(device) # (1, 1, D_pseudo, H, W)\n",
    "\n",
    "    # Adapt ROI size for sliding window if D_pseudo is smaller than Z-dim of inference_roi_size_base\n",
    "    roi_size_for_2d_adapted = list(config[\"inference_roi_size_base\"])\n",
    "    roi_size_for_2d_adapted[2] = min(roi_size_for_2d_adapted[2], d_pseudo) # Z-dim of ROI cannot exceed D_pseudo\n",
    "    # This ensures sliding window operates correctly within the limited depth.\n",
    "    # Alternatively, pad D_pseudo to match inference_roi_size_base[2] if it's larger.\n",
    "    # If d_pseudo < roi_size_for_2d_adapted[2], then pad input_tensor_pseudo_3d in Z dim.\n",
    "    # Let's assume d_pseudo >= minimal_z_depth for CNN or roi_size_for_2d_adapted[2] is handled.\n",
    "    # A simple approach: if d_pseudo < roi_size_for_2d_adapted[2], then the roi_z becomes d_pseudo (full depth).\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        base_model_output_pseudo_3d = sliding_window_inference(\n",
    "            input_tensor_pseudo_3d,\n",
    "            roi_size=tuple(roi_size_for_2d_adapted),\n",
    "            sw_batch_size=config[\"inference_sw_batch_size_base\"],\n",
    "            predictor=base_model,\n",
    "            overlap=config[\"inference_overlap_base\"],\n",
    "            mode=\"gaussian\",\n",
    "            progress=False # Less verbose for multiple slice calls\n",
    "        )\n",
    "    # base_model_output_pseudo_3d is (1, 1, D_pseudo, H, W)\n",
    "    base_prob_map_pseudo_3d = torch.sigmoid(base_model_output_pseudo_3d)\n",
    "    \n",
    "    # Extract central 2D probability map\n",
    "    central_slice_idx = d_pseudo // 2\n",
    "    # Squeeze out batch and channel, then take central Z slice: (H,W)\n",
    "    prob_map_2d = base_prob_map_pseudo_3d[0, 0, central_slice_idx, :, :].to(device) # Keep on device\n",
    "\n",
    "\n",
    "    if not gnn_model or not PYG_AVAILABLE: # If no GNN, use base model's 2D adapted output\n",
    "        print(f\"Performing 2D adapted inference using only base model for slice {slice_id}.\")\n",
    "        # Use 2D NMS for candidate nodes from prob_map_2d\n",
    "        # Need a 2D version of get_candidate_nodes_from_prob_map (or adapt existing)\n",
    "        # For simplicity, use existing by unsqueezing prob_map_2d to (1,H,W) and using NMS footprint (1, FH, FW)\n",
    "        candidate_coords_yx, candidate_probs_2d = get_candidate_nodes_from_prob_map(\n",
    "            prob_map_2d.unsqueeze(0), # Make it (1,H,W)\n",
    "            config[\"gnn_node_candidate_threshold\"],\n",
    "            (1, config[\"gnn_node_NMS_footprint\"][1], config[\"gnn_node_NMS_footprint\"][2]), # 2D NMS\n",
    "            max_candidates=200 # Limit per slice\n",
    "        ) # Returns (N,3) with Z-coord as 0. We need (N,2) for YX.\n",
    "        \n",
    "        candidate_coords_yx = candidate_coords_yx[:, 1:] # Keep only Y, X\n",
    "\n",
    "        detections_2d = []\n",
    "        for i, (y,x) in enumerate(candidate_coords_yx):\n",
    "            center_out = (float(x), float(y)) # X, Y order for output\n",
    "            bs_x, bs_y = config[\"output_bbox_size_2d\"]\n",
    "            bbox = (\n",
    "                x - (bs_x-1)//2, y - (bs_y-1)//2,\n",
    "                x + (bs_x-1)//2, y + (bs_y-1)//2\n",
    "            )\n",
    "            detections_2d.append({\n",
    "                \"center\": center_out, \"bbox\": tuple(map(float, bbox)), \n",
    "                \"score\": float(candidate_probs_2d[i]), \"id\": f\"{slice_id}_base_det_{i}\"})\n",
    "        return detections_2d\n",
    "\n",
    "\n",
    "    # --- 2. GNN Meta-Model Adaptation ---\n",
    "    # Candidate Generation (2D)\n",
    "    # Adapt NMS footprint for 2D: (1, FootprintY, FootprintX)\n",
    "    nms_footprint_2d = (1, config[\"gnn_node_NMS_footprint\"][1], config[\"gnn_node_NMS_footprint\"][2])\n",
    "    # get_candidate_nodes returns ZYX, so Z will be 0. We take YX.\n",
    "    candidate_coords_zyx_on_slice, _ = get_candidate_nodes_from_prob_map(\n",
    "        prob_map_2d.unsqueeze(0), # Add dummy Z dim for compatibility: (1,H,W)\n",
    "        config[\"gnn_node_candidate_threshold\"],\n",
    "        nms_footprint_2d\n",
    "    )\n",
    "    if candidate_coords_zyx_on_slice.shape[0] == 0:\n",
    "        # print(f\"No candidate nodes for GNN (2D adapted) on slice {slice_id}.\") # Can be too verbose\n",
    "        return []\n",
    "    \n",
    "    candidate_coords_yx = candidate_coords_zyx_on_slice[:, 1:] # Get (Y,X) coordinates\n",
    "\n",
    "    # Node Feature Extraction (2D) - use the 2D prob map\n",
    "    # extract_node_features expects (D,H,W) or (C,D,H,W) map and (Z,Y,X) coords\n",
    "    # We pass map (1,H,W) and coords (0,Y,X)\n",
    "    node_features_2d = extract_node_features(\n",
    "        np.insert(candidate_coords_yx, 0, 0, axis=1), # Add Z=0: (N,3) with (0,Y,X)\n",
    "        [prob_map_2d.unsqueeze(0)] # Pass list of maps [(1,H,W)]\n",
    "    )\n",
    "\n",
    "    # Graph Construction (2D) - use 2D proximity\n",
    "    # create_graph_edges expects (Z,Y,X). Pass (0,Y,X)\n",
    "    edge_index_2d = create_graph_edges(\n",
    "        np.insert(candidate_coords_yx, 0, 0, axis=1), # (N,3) with (0,Y,X)\n",
    "        config[\"gnn_edge_max_distance\"] # Use same distance, effectively becomes 2D dist\n",
    "    )\n",
    "\n",
    "    graph_for_gnn_2d = PyGData(\n",
    "        x=node_features_2d.to(device),\n",
    "        edge_index=edge_index_2d.to(device),\n",
    "        pos=torch.from_numpy(candidate_coords_yx).float().to(device) # Store (Y,X) positions\n",
    "    )\n",
    "    graph_for_gnn_2d.id_prefix = f\"{slice_id}_gnn_det\"\n",
    "\n",
    "    # GNN Inference\n",
    "    with torch.no_grad():\n",
    "        gnn_logits_2d = gnn_model(graph_for_gnn_2d.x, graph_for_gnn_2d.edge_index)\n",
    "        gnn_probs_2d = torch.sigmoid(gnn_logits_2d).squeeze(-1)\n",
    "    \n",
    "    graph_for_gnn_2d.pred_probs = gnn_probs_2d\n",
    "\n",
    "    # Post-process GNN outputs for 2D\n",
    "    gnn_final_score_threshold = config.get(\"gnn_final_score_threshold\", 0.5)\n",
    "    final_detections_2d = gnn_output_to_detections(\n",
    "        graph_for_gnn_2d,\n",
    "        gnn_final_score_threshold,\n",
    "        config[\"output_bbox_size_2d\"]\n",
    "    )\n",
    "    \n",
    "    return final_detections_2d\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 8: Hyperparameter Optimization with Optuna\n",
    "# This is a complex HPO setup because the GNN training depends on the base model.\n",
    "# The `objective` function for Optuna will:\n",
    "# 1.  Suggest hyperparameters for base model and GNN.\n",
    "# 2.  Run the full training pipeline (base model training, GNN data gen, GNN training).\n",
    "# 3.  Evaluate the *entire trained pipeline* on a final validation set (e.g., VAL_TOMO_IDS or a dedicated HPO validation set).\n",
    "# 4.  Return the F-beta score (beta=2.0).\n",
    "\n",
    "# %%\n",
    "# Global variable to store best trial results if needed, or use Optuna's study storage\n",
    "BEST_OPTUNA_RESULTS = {\n",
    "    \"best_f2_score\": -1.0,\n",
    "    \"best_trial_number\": -1,\n",
    "    \"best_params\": None\n",
    "}\n",
    "\n",
    "def objective_optuna(trial: optuna.trial.Trial):\n",
    "    # --- Suggest Hyperparameters ---\n",
    "    # Create a trial-specific config by overriding parts of global CONFIG\n",
    "    trial_config = CONFIG.copy() # Start with global defaults\n",
    "\n",
    "    # Base Model Hyperparameters\n",
    "    trial_config[\"base_model_lr\"] = trial.suggest_float(\"base_model_lr\", 1e-5, 1e-3, log=True)\n",
    "    # trial_config[\"patch_size_base\"] = ... # Could tune this if data loading adapts, but tricky. Keep fixed for now.\n",
    "    # trial_config[\"base_model_target_radius\"] = trial.suggest_int(\"base_model_target_radius\", 2, 5) # If this affects GT generation\n",
    "    # For DynUNet, can tune filters, depth, etc. This adds significant complexity.\n",
    "    # Example: num_unet_levels = trial.suggest_int(\"num_unet_levels\", 3, 5)\n",
    "    # dynunet_filters_start = trial.suggest_categorical(\"dynunet_filters_start\", [16, 32])\n",
    "    # trial_config[\"dynunet_filters\"] = [dynunet_filters_start * (2**i) for i in range(num_unet_levels)]\n",
    "    # trial_config[\"dynunet_strides\"] = ... # adapt based on num_unet_levels\n",
    "    # trial_config[\"unet_kernel_sizes\"] = ... # adapt based on num_unet_levels\n",
    "\n",
    "    # GNN Hyperparameters\n",
    "    trial_config[\"gnn_node_candidate_threshold\"] = trial.suggest_float(\"gnn_node_candidate_threshold\", 0.1, 0.7)\n",
    "    trial_config[\"gnn_edge_max_distance\"] = trial.suggest_int(\"gnn_edge_max_distance\", 10, 40)\n",
    "    trial_config[\"gnn_lr\"] = trial.suggest_float(\"gnn_gnn_lr\", 1e-4, 1e-2, log=True)\n",
    "    trial_config[\"gnn_hidden_channels\"] = trial.suggest_categorical(\"gnn_hidden_channels\", [32, 64, 128])\n",
    "    trial_config[\"gnn_num_layers\"] = trial.suggest_int(\"gnn_num_layers\", 2, 5)\n",
    "    trial_config[\"gnn_gat_heads\"] = trial.suggest_categorical(\"gnn_gat_heads\", [2, 4, 8])\n",
    "    # trial_config[\"gnn_final_score_threshold\"] = trial.suggest_float(\"gnn_final_score_threshold\", 0.3, 0.9) # Output threshold\n",
    "\n",
    "    # For demo, keep epochs low. In real HPO, these should also be reasonably high.\n",
    "    # trial_config[\"base_model_train_epochs\"] = trial.suggest_int(\"base_model_train_epochs\", 5, 20)\n",
    "    # trial_config[\"gnn_train_epochs\"] = trial.suggest_int(\"gnn_train_epochs\", 5, 20)\n",
    "    \n",
    "    print(f\"\\nOptuna Trial {trial.number}: Starting with params {trial.params}\")\n",
    "\n",
    "    # --- Run Full Training Pipeline ---\n",
    "    # This will use the trial_config which has the suggested HPs.\n",
    "    # The `train_base_model` and `train_gnn_model` functions internally handle Optuna trial reporting for pruning.\n",
    "    try:\n",
    "        trained_base_model, trained_gnn_model, pipeline_results_history = run_full_training_pipeline(trial_config, trial=trial)\n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        print(f\"Trial {trial.number} pruned.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        raise # Re-raise to Optuna\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed with error: {e}\")\n",
    "        # traceback.print_exc() # For detailed error\n",
    "        torch.cuda.empty_cache()\n",
    "        # Report a very bad score or let Optuna handle failure\n",
    "        return -1.0 # Or some other indicator of failure that Optuna minimizes/maximizes away from\n",
    "\n",
    "    if trained_base_model is None or (CONFIG[\"gnn_model_name\"] and trained_gnn_model is None and PYG_AVAILABLE):\n",
    "        print(f\"Trial {trial.number}: Model training failed to produce models. Reporting low score.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return -1.0 # Or appropriate low score for maximization problem\n",
    "\n",
    "    # --- Evaluate Entire Pipeline on a Hold-out Validation Set ---\n",
    "    # This validation set should NOT have been used for GNN training data generation if possible.\n",
    "    # Or, use the same VAL_TOMO_IDS, acknowledging this is a validation of GNN's performance on data derived from these.\n",
    "    # For a robust HPO, a dedicated HPO_VAL_TOMO_IDS set is best.\n",
    "    # Here, we'll use VAL_TOMO_IDS for simplicity for final evaluation.\n",
    "    \n",
    "    if not VAL_TOMO_IDS:\n",
    "        print(f\"Trial {trial.number}: No validation tomograms for final Optuna evaluation. Returning GNN's internal best val metric if available.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return pipeline_results_history.get(\"final_gnn_val_metric\", 0.0) if pipeline_results_history else 0.0\n",
    "\n",
    "    print(f\"Trial {trial.number}: Evaluating pipeline on validation set: {VAL_TOMO_IDS}\")\n",
    "    all_tomogram_f2_scores = []\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "    for tomo_id_val in VAL_TOMO_IDS:\n",
    "        # Perform 3D inference\n",
    "        detections = inference_on_3d_tomogram(\n",
    "            tomo_id_val, trained_base_model, trained_gnn_model,\n",
    "            trial_config, trial_config[\"dataset_root\"], trial_config[\"device\"]\n",
    "        )\n",
    "        \n",
    "        # Match detections to GT for this tomogram\n",
    "        # Define matching threshold - could also be part of HPO if desired\n",
    "        matching_dist_3d = trial_config[\"base_model_target_radius\"] * 3 # Heuristic\n",
    "        \n",
    "        eval_results_tomo = evaluate_pipeline_on_tomogram(\n",
    "            tomo_id_val, detections, trial_config[\"gt_csv_path\"], matching_dist_3d, beta=2.0\n",
    "        )\n",
    "        all_tomogram_f2_scores.append(eval_results_tomo[CONFIG['optuna_objective_metric']])\n",
    "        total_tp += eval_results_tomo['tp']\n",
    "        total_fp += eval_results_tomo['fp']\n",
    "        total_fn += eval_results_tomo['fn']\n",
    "        print(f\"Tomo {tomo_id_val} - F2: {eval_results_tomo[CONFIG['optuna_objective_metric']]:.4f}, P: {eval_results_tomo['precision']:.4f}, R: {eval_results_tomo['recall']:.4f}\")\n",
    "\n",
    "    # Aggregate metric: either average F2 over tomograms, or calculate F2 from total TP/FP/FN\n",
    "    if all_tomogram_f2_scores:\n",
    "        # Micro-averaged F2 from total TP, FP, FN\n",
    "        final_f2_score, _, _ = calculate_fbeta_precision_recall(\n",
    "             np.concatenate([np.ones(total_tp), np.ones(total_fp), np.zeros(total_fn)]),\n",
    "             np.concatenate([np.ones(total_tp), np.zeros(total_fp), np.ones(total_fn)]),\n",
    "             beta=2.0\n",
    "        )\n",
    "        # final_f2_score = np.mean(all_tomogram_f2_scores) # Macro-average\n",
    "    else:\n",
    "        final_f2_score = 0.0 # Default if no validation occurred\n",
    "\n",
    "    print(f\"Trial {trial.number}: Overall F2-score on validation set: {final_f2_score:.4f}\")\n",
    "\n",
    "    # Store best trial info (optional, Optuna study object also stores this)\n",
    "    if final_f2_score > BEST_OPTUNA_RESULTS[\"best_f2_score\"]:\n",
    "        BEST_OPTUNA_RESULTS[\"best_f2_score\"] = final_f2_score\n",
    "        BEST_OPTUNA_RESULTS[\"best_trial_number\"] = trial.number\n",
    "        BEST_OPTUNA_RESULTS[\"best_params\"] = trial.params\n",
    "        # Save best models from this trial (can get very disk intensive)\n",
    "        # Consider saving just the trial_config or a reference.\n",
    "    \n",
    "    torch.cuda.empty_cache() # Clean up GPU memory after trial\n",
    "    return final_f2_score\n",
    "\n",
    "\n",
    "def run_optuna_study():\n",
    "    if not TRAIN_TOMO_IDS or not VAL_TOMO_IDS:\n",
    "        print(\"Optuna study cannot run: Missing training or validation tomogram IDs.\")\n",
    "        return None, None\n",
    "\n",
    "    study_name = f\"{CONFIG['project_name']}_study\"\n",
    "    # Ensure output_dir is a string for f-string formatting if it was converted to Path earlier\n",
    "    output_dir_str = str(CONFIG[\"output_dir\"]) \n",
    "    storage_path = f\"sqlite:///{Path(output_dir_str) / study_name}.db\" # Save study to a DB\n",
    "\n",
    "    # Check if PyG is available; if not, GNN part is dummied, so HPO for GNN params is less meaningful\n",
    "    if not PYG_AVAILABLE and CONFIG[\"gnn_model_name\"]:\n",
    "        print(\"WARNING: PyTorch Geometric not available. GNN-related HPO will be for a dummy GNN.\")\n",
    "\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"maximize\", # We want to maximize F2-score\n",
    "        pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=2, interval_steps=1), # Prune early if not promising\n",
    "        storage=storage_path,\n",
    "        load_if_exists=True # Resume study if it exists\n",
    "    )\n",
    "\n",
    "    # <<< BEGIN MODIFICATION >>>\n",
    "    import json \n",
    "    import copy\n",
    "    # Ensure torch and Path are accessible here, they should be from global imports\n",
    "    # from pathlib import Path # Already imported globally\n",
    "    # import torch # Already imported globally\n",
    "\n",
    "    def make_json_serializable(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [make_json_serializable(i) for i in obj]\n",
    "        elif isinstance(obj, tuple): # Tuples also need to be converted if they contain non-serializable items\n",
    "            return tuple(make_json_serializable(i) for i in obj)\n",
    "        elif isinstance(obj, torch.device):\n",
    "            return str(obj)\n",
    "        elif isinstance(obj, Path):\n",
    "            return str(obj)\n",
    "        # Add other non-serializable types here if necessary\n",
    "        # Common types like int, float, str, bool, None are already serializable\n",
    "        return obj\n",
    "\n",
    "    serializable_config = make_json_serializable(copy.deepcopy(CONFIG))\n",
    "    # <<< END MODIFICATION >>>\n",
    "\n",
    "    # Set user attributes for the study to store the main config (without trial-specific changes)\n",
    "    study.set_user_attr(\"global_config\", serializable_config) # Use the modified, serializable copy\n",
    "\n",
    "    try:\n",
    "        study.optimize(objective_optuna, n_trials=CONFIG[\"optuna_n_trials\"], timeout=None) # Add timeout if desired\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Optuna study interrupted by user.\")\n",
    "    except Exception as e: # Catch other potential errors during optimize\n",
    "        print(f\"Exception during study.optimize: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\nOptuna Study Summary:\")\n",
    "    print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "    \n",
    "    best_trial = None\n",
    "    try:\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"  Best trial number: {best_trial.number}\")\n",
    "        print(f\"  Best F2-score: {best_trial.value:.4f}\")\n",
    "        print(\"  Best parameters:\")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        \n",
    "        # Save best params to a JSON file\n",
    "        best_params_path = Path(str(CONFIG[\"output_dir\"])) / \"best_optuna_params.json\" # Ensure CONFIG[\"output_dir\"] is string\n",
    "        with open(best_params_path, \"w\") as f:\n",
    "            json.dump(best_trial.params, f, indent=4)\n",
    "        print(f\"Best parameters saved to {best_params_path}\")\n",
    "\n",
    "    except ValueError: # If no trials completed or all failed\n",
    "        print(\"  No successful trials completed in the study.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error retrieving best trial: {e}\")\n",
    "\n",
    "    return study, best_trial\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 9: Usage Instructions\n",
    "#\n",
    "# 1.  **Setup:**\n",
    "#     *   Install all required packages as listed in \"Part 0: Setup and Imports\". Ensure your PyTorch installation matches your CUDA version. PyTorch Geometric installation might require specific commands based on your PyTorch/CUDA versions.\n",
    "#     *   Make sure a CUDA-enabled GPU is available and selected by PyTorch (`CONFIG[\"device\"]`).\n",
    "#\n",
    "# 2.  **Data Structure:**\n",
    "#     *   Set `CONFIG[\"dataset_root\"]` to the main directory containing your tomogram data.\n",
    "#     *   Inside `CONFIG[\"dataset_root\"]`, each tomogram should be a subfolder (e.g., `tomo_001acc`, `tomo_002xyz`).\n",
    "#     *   Each tomogram subfolder must contain its 2D slices as sequentially named JPG files (e.g., `slice_0000.jpg`, `slice_0001.jpg`, ...).\n",
    "#     *   Set `CONFIG[\"gt_csv_path\"]` to the path of your ground truth CSV file.\n",
    "#     *   The CSV file must contain at least the columns: `tomo_id` (matching folder names), `center_x`, `center_y`, `center_z` (voxel coordinates of motor centers).\n",
    "#     *   The simulation code (`simulate_dataset`) can be used to create a dummy dataset if you don't have one initially. Comment it out for real data.\n",
    "#     *   Define `TRAIN_TOMO_IDS`, `VAL_TOMO_IDS`, `TEST_TOMO_IDS` lists with the respective tomogram IDs.\n",
    "#\n",
    "# 3.  **Configuration:**\n",
    "#     *   Review and adjust parameters in `CONFIG` (Part 1), especially:\n",
    "#         *   `patch_size_base`: Critical for base model training. Ensure it fits your GPU VRAM.\n",
    "#         *   `*_train_epochs`: Increase for real training (e.g., 100+ for base, 50+ for GNN).\n",
    "#         *   `optuna_n_trials`: Increase significantly for meaningful HPO (e.g., 50-100+).\n",
    "#         *   `output_dir`: Where models and results will be saved.\n",
    "#\n",
    "# 4.  **Running Hyperparameter Optimization (HPO - Recommended First Step):**\n",
    "#     ```python\n",
    "#     # Make sure all necessary data loaders (train_loader_base, val_loader_base) are created\n",
    "#     # by running the cells in Part 2 after configuring paths and IDs.\n",
    "#     # Then run:\n",
    "#     # optuna_study, optuna_best_trial = run_optuna_study()\n",
    "#     # if optuna_best_trial:\n",
    "#     #     print(\"\\nTo train with best HPO params, update CONFIG with optuna_best_trial.params and re-run training.\")\n",
    "#     #     CONFIG.update(optuna_best_trial.params) # Update global CONFIG with best params\n",
    "#     #     # Or create a new config from best_trial.params\n",
    "#     #     # best_config = CONFIG.copy()\n",
    "#     #     # best_config.update(optuna_best_trial.params)\n",
    "#     ```\n",
    "#     *   This will run the Optuna study. Results (including best parameters) will be saved.\n",
    "#     *   After HPO, update `CONFIG` with the `best_trial.params` found by Optuna for final model training.\n",
    "#\n",
    "# 5.  **Running Training (with fixed or HPO-tuned config):**\n",
    "#     ```python\n",
    "#     # Ensure CONFIG has desired parameters (either default or from HPO)\n",
    "#     # Ensure data loaders (train_loader_base, val_loader_base) are created.\n",
    "#     # Then run:\n",
    "#     # trained_base_model, trained_gnn_model, pipeline_history = run_full_training_pipeline(CONFIG)\n",
    "#     # if trained_base_model and trained_gnn_model:\n",
    "#     #     print(\"Training complete. Models are ready for inference.\")\n",
    "#     #     # Save final models explicitly if not done by training loops or if you want a specific name\n",
    "#     #     final_base_model_path = Path(CONFIG[\"output_dir\"]) / \"final_base_model.pth\"\n",
    "#     #     final_gnn_model_path = Path(CONFIG[\"output_dir\"]) / \"final_gnn_model.pth\"\n",
    "#     #     torch.save(trained_base_model.state_dict(), final_base_model_path)\n",
    "#     #     if PYG_AVAILABLE and trained_gnn_model:\n",
    "#     #         torch.save(trained_gnn_model.state_dict(), final_gnn_model_path)\n",
    "#     #     print(f\"Final models saved to {final_base_model_path} and {final_gnn_model_path}\")\n",
    "#     # elif trained_base_model:\n",
    "#     #     print(\"Base model training complete. GNN training might have been skipped or failed.\")\n",
    "#     #     final_base_model_path = Path(CONFIG[\"output_dir\"]) / \"final_base_model.pth\"\n",
    "#     #     torch.save(trained_base_model.state_dict(), final_base_model_path)\n",
    "#     #     print(f\"Final base model saved to {final_base_model_path}\")\n",
    "#     # else:\n",
    "#     #     print(\"Training failed.\")\n",
    "#     ```\n",
    "#\n",
    "# 6.  **Running Inference:**\n",
    "#     *   Load your trained models:\n",
    "#         ```python\n",
    "#         # # Example loading (adjust paths and model instantiation as needed)\n",
    "#         # inference_config = CONFIG.copy() # Use the config the models were trained with\n",
    "#         #\n",
    "#         # # Load Base Model\n",
    "#         # loaded_base_model = get_base_model(inference_config) # Re-create model structure\n",
    "#         # # Find the best saved base model checkpoint, e.g. from HPO or final training\n",
    "#         # # base_model_checkpoint_path = Path(inference_config[\"output_dir\"]) / \"DynUNet_best_epochX_diceY.pth\" # Or final_base_model.pth\n",
    "#         # # For example, if HPO was run and best_params.json exists:\n",
    "#         # best_params_json_path = Path(inference_config[\"output_dir\"]) / \"best_optuna_params.json\"\n",
    "#         # if best_params_json_path.exists():\n",
    "#         #    with open(best_params_json_path, 'r') as f:\n",
    "#         #        best_hpo_params = json.load(f)\n",
    "#         #        inference_config.update(best_hpo_params) # Ensure model structure matches\n",
    "#         #    # Need to find the actual .pth file associated with the best HPO trial,\n",
    "#         #    # This example assumes you saved a \"final_base_model.pth\" after HPO-informed training.\n",
    "#         #    base_model_checkpoint_path = Path(inference_config[\"output_dir\"]) / \"final_base_model.pth\"\n",
    "#         # else: # Fallback if no HPO params found - try to find a recently saved model\n",
    "#         #    # This part needs robust logic to find the correct model file.\n",
    "#         #    # For demo, assuming a fixed name if run_full_training_pipeline was used:\n",
    "#         #    base_model_checkpoint_path = Path(inference_config[\"output_dir\"]) / \"final_base_model.pth\"\n",
    "#         #\n",
    "#         # if base_model_checkpoint_path.exists():\n",
    "#         #    loaded_base_model.load_state_dict(torch.load(base_model_checkpoint_path, map_location=inference_config[\"device\"]))\n",
    "#         #    print(f\"Loaded base model from {base_model_checkpoint_path}\")\n",
    "#         # else:\n",
    "#         #    print(f\"ERROR: Base model checkpoint {base_model_checkpoint_path} not found for inference.\")\n",
    "#         #    # loaded_base_model = None # Or handle error\n",
    "#         #\n",
    "#         # # Load GNN Model (if used)\n",
    "#         # loaded_gnn_model = None\n",
    "#         # if PYG_AVAILABLE and inference_config[\"gnn_model_name\"]:\n",
    "#         #    # Determine gnn_in_channels. This is tricky without knowing the base model outputs during that training.\n",
    "#         #    # Assuming it's based on num_base_models for simplicity.\n",
    "#         #    gnn_in_channels_inf = inference_config[\"num_base_models\"] # Matching the training setup\n",
    "#         #    loaded_gnn_model = GATNet(\n",
    "#         #        in_channels=gnn_in_channels_inf,\n",
    "#         #        hidden_channels=inference_config[\"gnn_hidden_channels\"], # From loaded best_params or default\n",
    "#         #        out_channels=1,\n",
    "#         #        num_layers=inference_config[\"gnn_num_layers\"],\n",
    "#         #        heads=inference_config[\"gnn_gat_heads\"]\n",
    "#         #    )\n",
    "#         #    # gnn_model_checkpoint_path = Path(inference_config[\"output_dir\"]) / \"GAT_best_f2_X.pth\" # Or final_gnn_model.pth\n",
    "#         #    gnn_model_checkpoint_path = Path(inference_config[\"output_dir\"]) / \"final_gnn_model.pth\"\n",
    "#         #    if gnn_model_checkpoint_path.exists():\n",
    "#         #        loaded_gnn_model.load_state_dict(torch.load(gnn_model_checkpoint_path, map_location=inference_config[\"device\"]))\n",
    "#         #        print(f\"Loaded GNN model from {gnn_model_checkpoint_path}\")\n",
    "#         #    else:\n",
    "#         #        print(f\"WARNING: GNN model checkpoint {gnn_model_checkpoint_path} not found. GNN will not be used or may error.\")\n",
    "#         #        # loaded_gnn_model = None\n",
    "#         ```\n",
    "#     *   **Mode 1: Inference on Full 3D Tomogram:**\n",
    "#         ```python\n",
    "#         # if loaded_base_model: # Check if base model loaded successfully\n",
    "#         #    tomo_id_to_infer = TEST_TOMO_IDS[0] # Example\n",
    "#         #    detections_3d = inference_on_3d_tomogram(\n",
    "#         #        tomo_id_to_infer, loaded_base_model, loaded_gnn_model,\n",
    "#         #        inference_config, inference_config[\"dataset_root\"], inference_config[\"device\"]\n",
    "#         #    )\n",
    "#         #    print(f\"3D Detections for {tomo_id_to_infer}: {detections_3d}\")\n",
    "#         #\n",
    "#         #    # Optional: Evaluate if ground truth is available for test tomogram\n",
    "#         #    # eval_3d = evaluate_pipeline_on_tomogram(tomo_id_to_infer, detections_3d, inference_config[\"gt_csv_path\"], matching_dist_thresh_3d=10)\n",
    "#         #    # print(f\"Evaluation for {tomo_id_to_infer}: {eval_3d}\")\n",
    "#         ```\n",
    "#     *   **Mode 2: Inference on Individual 2D Slice:**\n",
    "#         ```python\n",
    "#         # if loaded_base_model: # Check if base model loaded successfully\n",
    "#         #    # Load a sample 2D slice (e.g., first slice of first test tomogram)\n",
    "#         #    tomo_id_for_slice = TEST_TOMO_IDS[0]\n",
    "#         #    slice_path_pattern = str(Path(inference_config[\"dataset_root\"]) / tomo_id_for_slice / \"slice_*.jpg\")\n",
    "#         #    slice_files = sorted(glob.glob(slice_path_pattern))\n",
    "#         #    if slice_files:\n",
    "#         #        sample_slice_path = slice_files[len(slice_files)//2] # Middle slice\n",
    "#         #        sample_2d_image_hw = cv2.imread(sample_slice_path, cv2.IMREAD_GRAYSCALE)\n",
    "#         #        sample_slice_id = Path(sample_slice_path).stem\n",
    "#         #\n",
    "#         #        detections_2d = inference_on_2d_slice(\n",
    "#         #            sample_2d_image_hw, sample_slice_id,\n",
    "#         #            loaded_base_model, loaded_gnn_model,\n",
    "#         #            inference_config, inference_config[\"device\"]\n",
    "#         #        )\n",
    "#         #        print(f\"2D Detections for slice {sample_slice_id}: {detections_2d}\")\n",
    "#         #    else:\n",
    "#         #        print(f\"No slices found for tomogram {tomo_id_for_slice} to test 2D inference.\")\n",
    "#         ```\n",
    "#\n",
    "# 7.  **Interpreting Output:**\n",
    "#     *   Detections are lists of dictionaries, each containing `center`, `bbox`, and `score`.\n",
    "#     *   Coordinates are (X,Y,Z) for 3D and (X,Y) for 2D. Bounding boxes are (X_min,Y_min,Z_min,X_max,Y_max,Z_max) or (X_min,Y_min,X_max,Y_max).\n",
    "#\n",
    "# **Important Notes for Execution:**\n",
    "# *   **GPU Memory:** This pipeline, especially with DynUNet and GNNs on large graphs, is VRAM intensive. Monitor GPU usage. Reduce `patch_size_base`, `base_model_batch_size`, or GNN complexity if OOM errors occur.\n",
    "# *   **Time:** Full training and HPO will take a very long time. The demo settings (`*_train_epochs`, `optuna_n_trials`) are minimal.\n",
    "# *   **Model Saving/Loading:** The provided training loops save best models. Ensure the loading logic in inference correctly identifies and loads these models along with the configuration they were trained with (especially for model architecture HPs like GNN layers/hidden_dims). The example inference loading is basic and may need refinement for robustly finding the correct HPO-derived model.\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 10: Discussion, Challenges, and Future Work\n",
    "#\n",
    "# ### Discussion\n",
    "# This notebook outlines a comprehensive pipeline for 3D bacterial motor detection using a GNN-stacked ensemble. The core idea is to leverage 3D CNNs for powerful feature extraction from cryo-ET data and then use a GNN to intelligently combine and refine these predictions, considering spatial relationships between candidate detections. The inclusion of Optuna for HPO is critical for navigating the large parameter space of such a multi-stage model. The adaptation for 2D slice-based inference, while challenging, significantly increases the pipeline's versatility.\n",
    "#\n",
    "# ### Key Challenges Addressed (and inherent complexities)\n",
    "# *   **Designing Effective GNN Node Features:** Node features are derived from base CNN probability maps. This is a common starting point. More sophisticated features could include embeddings from intermediate CNN layers, or geometric/intensity statistics of candidate regions, but this increases complexity.\n",
    "# *   **Managing Training and HPO Complexity:** The two-stage training (base models then GNN) is standard for stacking. HPO over the entire pipeline is computationally expensive. Each Optuna trial involves re-training base models and then the GNN. Pruning strategies in Optuna are essential. Efficient data caching (e.g., `PersistentDataset` for base model training, pre-generating GNN graph data if base models are fixed) can save time.\n",
    "# *   **GNN Adaptation for 2D Inference:** This is a novel aspect. The primary challenge is ensuring the GNN, trained on 3D graph structures and features, can generalize to graphs derived from pseudo-3D/2D information. Success hinges on:\n",
    "#     *   The robustness of the GNN architecture (GATs are generally good with varying graph structures).\n",
    "#     *   The semantic consistency of node features between 3D and adapted 2D inputs. If features are primarily \"probability of motor,\" this should hold.\n",
    "#     *   The quality of the pseudo-3D volume generation for the base model.\n",
    "# *   **Robust Detection of Very Small Motors:** This is an inherent challenge in cryo-ET.\n",
    "#     *   Base CNNs need appropriate receptive fields and capacity. FocalLoss or DiceCELoss can help with class imbalance.\n",
    "#     *   High-resolution patches and careful augmentation are important.\n",
    "#     *   The GNN's ability to filter false positives based on relational context is a key benefit here.\n",
    "# *   **Computational Resources:** High-end GPUs with substantial VRAM (>=16-24GB) are necessary, especially for training with large 3D patches and GNNs on potentially large graphs.\n",
    "#\n",
    "# ### Limitations\n",
    "# *   **Single Notebook Constraint:** A real-world project of this scale would typically be modularized into multiple Python scripts for better organization, testing, and maintainability.\n",
    "# *   **Data Simulation:** The provided data simulation is basic. Real cryo-ET data has complex noise and structural characteristics.\n",
    "# *   **HPO Scope:** The HPO in this notebook tunes a subset of possible parameters. A more exhaustive search could tune DynUNet architectural details, GNN edge construction rules, GNN node feature selection, etc.\n",
    "# *   **GNN Graph Size:** For very large tomograms generating many candidate nodes, GNN training/inference could become a bottleneck. Graph sampling or hierarchical approaches might be needed.\n",
    "# *   **Ground Truth Assumption:** Assumes point center GT. If GT provides bounding boxes or segmentation masks, the target generation for base models and evaluation could be more precise.\n",
    "# *   **Matching Criteria:** The greedy matching for evaluation is standard but not always optimal.\n",
    "#\n",
    "# ### Future Improvements\n",
    "# *   **Advanced Base Models:** Explore other 3D architectures or pre-trained models if applicable. Use multiple, diverse base models for the ensemble.\n",
    "# *   **Richer GNN Node Features:** Incorporate learnable embeddings from base CNNs (e.g., features from bottleneck layers corresponding to candidate locations) or more handcrafted geometric/intensity features.\n",
    "# *   **Hierarchical GNNs:** For multi-scale analysis if motor appearance varies significantly with scale or context.\n",
    "# *   **End-to-End Trainability (Advanced):** While complex, exploring methods to jointly optimize (or fine-tune) base models and the GNN in an end-to-end fashion could potentially improve performance but significantly increases difficulty.\n",
    "# *   **More Sophisticated 2D Adaptation:**\n",
    "#     *   Experiment with different pseudo-3D volume creation strategies (e.g., padding with learned values, using adjacent real slices if available in a sequence).\n",
    "#     *   Train a dedicated \"adapter\" module or fine-tune the GNN slightly on 2D-derived graphs if sufficient 2D labeled data or a good simulation thereof exists.\n",
    "# *   **Uncertainty Quantification:** Estimate confidence scores that reflect true uncertainty, potentially using Bayesian deep learning techniques or ensemble disagreement.\n",
    "# *   **Active Learning:** If labeling is a bottleneck, incorporate active learning to select the most informative tomograms/regions for annotation.\n",
    "# *   **Interpretability:** Use GNN explainability methods (e.g., GNNExplainer) to understand which nodes/edges are most important for the GNN's decisions, providing insights into the detection process.\n",
    "#\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# # Main Execution Block (Example Workflow)\n",
    "# ---\n",
    "# This block demonstrates a possible sequence of operations.\n",
    "# You would typically run HPO first, then use the best parameters for final training and inference.\n",
    "# For this demonstration, we'll run a very short version.\n",
    "# **Comment out or select parts to run as needed.**\n",
    "\n",
    "# %%\n",
    "if __name__ == '__main__':\n",
    "    # Ensure data loaders are initialized if needed for any step\n",
    "    # This happens when their respective cells are run.\n",
    "\n",
    "    # --- OPTION 1: Run Hyperparameter Optimization ---\n",
    "    print(\"WORKFLOW: Starting Hyperparameter Optimization (Optuna)...\")\n",
    "    # Make sure train_loader_base and val_loader_base are initialized by running Part 2 cells\n",
    "    if train_loader_base is None or val_loader_base is None:\n",
    "         print(\"ERROR: DataLoaders not initialized. Cannot run Optuna. Please run dataset/dataloader cells first.\")\n",
    "    else:\n",
    "        # Reduce epochs for HPO demo to make it faster\n",
    "        original_base_epochs = CONFIG[\"base_model_train_epochs\"]\n",
    "        original_gnn_epochs = CONFIG[\"gnn_train_epochs\"]\n",
    "        CONFIG[\"base_model_train_epochs\"] = 1 # Minimal for HPO demo\n",
    "        CONFIG[\"gnn_train_epochs\"] = 1       # Minimal for HPO demo\n",
    "        CONFIG[\"optuna_n_trials\"] = 2       # Very few trials for demo\n",
    "\n",
    "        optuna_study, optuna_best_trial = run_optuna_study()\n",
    "\n",
    "        # Restore original epochs if changed for HPO demo\n",
    "        CONFIG[\"base_model_train_epochs\"] = original_base_epochs\n",
    "        CONFIG[\"gnn_train_epochs\"] = original_gnn_epochs\n",
    "        \n",
    "        if optuna_best_trial:\n",
    "            print(\"\\nOptuna finished. Best parameters found:\")\n",
    "            print(json.dumps(optuna_best_trial.params, indent=2))\n",
    "            print(f\"Best F2-score from HPO: {optuna_best_trial.value}\")\n",
    "            # Update global CONFIG with these best parameters for subsequent training/inference\n",
    "            CONFIG.update(optuna_best_trial.params)\n",
    "            print(\"Global CONFIG updated with best HPO parameters.\")\n",
    "        else:\n",
    "            print(\"\\nOptuna study did not yield a best trial. Using default CONFIG for subsequent steps.\")\n",
    "    \n",
    "    # --- OPTION 2: Run Full Training with Current CONFIG ---\n",
    "    # (This could be the CONFIG updated by HPO, or default if HPO was skipped)\n",
    "    print(\"\\nWORKFLOW: Starting Full Training Pipeline with current CONFIG...\")\n",
    "    # Ensure data loaders are initialized\n",
    "    if train_loader_base is None or val_loader_base is None:\n",
    "         print(\"ERROR: DataLoaders not initialized. Cannot run full training. Please run dataset/dataloader cells first.\")\n",
    "    else:\n",
    "        # Use slightly more epochs for this \"final\" training run than in HPO demo\n",
    "        CONFIG[\"base_model_train_epochs\"] = 2 # Still low for notebook demo\n",
    "        CONFIG[\"gnn_train_epochs\"] = 2       # Still low for notebook demo\n",
    "\n",
    "        trained_base_model, trained_gnn_model, pipeline_history = run_full_training_pipeline(CONFIG)\n",
    "        \n",
    "        if trained_base_model: # At least base model should be trained\n",
    "            print(\"Full training pipeline complete (or partially complete if GNN failed).\")\n",
    "            # Save final models\n",
    "            final_base_model_path = Path(CONFIG[\"output_dir\"]) / \"final_trained_base_model.pth\"\n",
    "            torch.save(trained_base_model.state_dict(), final_base_model_path)\n",
    "            print(f\"Saved final base model to {final_base_model_path}\")\n",
    "\n",
    "            if trained_gnn_model and PYG_AVAILABLE and CONFIG[\"gnn_model_name\"]:\n",
    "                final_gnn_model_path = Path(CONFIG[\"output_dir\"]) / \"final_trained_gnn_model.pth\"\n",
    "                torch.save(trained_gnn_model.state_dict(), final_gnn_model_path)\n",
    "                print(f\"Saved final GNN model to {final_gnn_model_path}\")\n",
    "            elif CONFIG[\"gnn_model_name\"] and not PYG_AVAILABLE:\n",
    "                print(\"GNN model specified in config, but PyG not available. GNN part was dummied.\")\n",
    "            elif CONFIG[\"gnn_model_name\"] and not trained_gnn_model:\n",
    "                 print(\"GNN model specified and PyG available, but GNN model not trained/returned.\")\n",
    "\n",
    "            # --- OPTION 3: Run Inference (after training or loading models) ---\n",
    "            print(\"\\nWORKFLOW: Starting Inference...\")\n",
    "            if not TEST_TOMO_IDS:\n",
    "                print(\"No test tomogram IDs defined. Skipping inference.\")\n",
    "            else:\n",
    "                # Use the models just trained, or implement loading logic here if running separately\n",
    "                # For simplicity, use models from the training step above.\n",
    "                # If running inference in a new session, you'd load them:\n",
    "                # loaded_base_model = get_base_model(CONFIG)\n",
    "                # loaded_base_model.load_state_dict(torch.load(final_base_model_path, map_location=CONFIG[\"device\"]))\n",
    "                # if PYG_AVAILABLE and CONFIG[\"gnn_model_name\"]:\n",
    "                #    gnn_in_channels_inf = ... # determine correctly\n",
    "                #    loaded_gnn_model = GATNet(...)\n",
    "                #    loaded_gnn_model.load_state_dict(torch.load(final_gnn_model_path, map_location=CONFIG[\"device\"]))\n",
    "                # else:\n",
    "                #    loaded_gnn_model = None\n",
    "\n",
    "                # Use `trained_base_model` and `trained_gnn_model` directly for this demo flow.\n",
    "                \n",
    "                # Mode 1: 3D Tomogram Inference\n",
    "                tomo_id_to_infer_3d = TEST_TOMO_IDS[0]\n",
    "                print(f\"\\nRunning 3D inference on tomogram: {tomo_id_to_infer_3d}\")\n",
    "                detections_3d = inference_on_3d_tomogram(\n",
    "                    tomo_id_to_infer_3d, trained_base_model, trained_gnn_model,\n",
    "                    CONFIG, CONFIG[\"dataset_root\"], CONFIG[\"device\"]\n",
    "                )\n",
    "                print(f\"Found {len(detections_3d)} detections in 3D for {tomo_id_to_infer_3d}.\")\n",
    "                # for det_3d in detections_3d[:min(3, len(detections_3d))]: print(det_3d) # Print a few\n",
    "\n",
    "                # Evaluate 3D detections if GT available for test set\n",
    "                eval_3d = evaluate_pipeline_on_tomogram(\n",
    "                    tomo_id_to_infer_3d, detections_3d, CONFIG[\"gt_csv_path\"], \n",
    "                    matching_dist_thresh_3d=CONFIG[\"base_model_target_radius\"] * 3, beta=2.0\n",
    "                )\n",
    "                print(f\"Evaluation for 3D detections on {tomo_id_to_infer_3d}: {eval_3d}\")\n",
    "\n",
    "\n",
    "                # Mode 2: 2D Slice Inference\n",
    "                tomo_id_for_slice_2d = TEST_TOMO_IDS[0]\n",
    "                slice_path_pattern_2d = str(Path(CONFIG[\"dataset_root\"]) / tomo_id_for_slice_2d / \"slice_*.jpg\")\n",
    "                slice_files_2d = sorted(glob.glob(slice_path_pattern_2d))\n",
    "                \n",
    "                if slice_files_2d:\n",
    "                    # Pick a slice for demo, e.g., middle slice\n",
    "                    sample_slice_path_2d = slice_files_2d[len(slice_files_2d) // 2]\n",
    "                    sample_2d_image_hw = cv2.imread(sample_slice_path_2d, cv2.IMREAD_GRAYSCALE)\n",
    "                    sample_slice_id_2d = Path(sample_slice_path_2d).stem\n",
    "                    print(f\"\\nRunning 2D (adapted) inference on slice: {sample_slice_id_2d} from {tomo_id_for_slice_2d}\")\n",
    "\n",
    "                    detections_2d = inference_on_2d_slice(\n",
    "                        sample_2d_image_hw, sample_slice_id_2d,\n",
    "                        trained_base_model, trained_gnn_model,\n",
    "                        CONFIG, CONFIG[\"device\"]\n",
    "                    )\n",
    "                    print(f\"Found {len(detections_2d)} detections in 2D for slice {sample_slice_id_2d}.\")\n",
    "                    # for det_2d in detections_2d[:min(3, len(detections_2d))]: print(det_2d) # Print a few\n",
    "                else:\n",
    "                    print(f\"No slices found for tomogram {tomo_id_for_slice_2d} to test 2D inference.\")\n",
    "        else:\n",
    "            print(\"Base model training failed or was skipped. Cannot proceed to inference with trained models.\")\n",
    "\n",
    "    print(\"\\nCryoEM-MotorMetaNet Workflow Demo Finished.\")\n",
    "\n",
    "# %%\n",
    "# To run the main execution block when executing the notebook:\n",
    "# You would typically uncomment the call to if __name__ == '__main__':\n",
    "# and run the cell. For safety in a notebook that defines functions and then calls them,\n",
    "# it's often better to call the main execution logic explicitly in a new cell.\n",
    "# Example:\n",
    "#\n",
    "# if train_loader_base and val_loader_base: # Check if data setup was done\n",
    "#    # Call optuna part\n",
    "#    optuna_study, optuna_best_trial = run_optuna_study()\n",
    "#    if optuna_best_trial:\n",
    "#        CONFIG.update(optuna_best_trial.params)\n",
    "#\n",
    "#    # Call training part\n",
    "#    trained_base_model, trained_gnn_model, pipeline_history = run_full_training_pipeline(CONFIG)\n",
    "#\n",
    "#    # Call inference part with trained_base_model, trained_gnn_model\n",
    "# else:\n",
    "#    print(\"Please run data setup cells (Part 2) before running main workflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9873b0e-0297-45c5-9558-f9a0eab1d5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
